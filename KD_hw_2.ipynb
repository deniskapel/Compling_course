{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwUaGt5avGb_"
   },
   "source": [
    "# Task 1\n",
    "В тетрадке реализована биграмная языковая модель (при генерации учитывается информация только о 1 предыдущем слове). Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами, сгенерированными биграмной моделью. \n",
    "Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке).  \n",
    "\n",
    "Делать это задание будет легче после прочтения первых 7 страниц вот этой главы из Журафского - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Y1CnpfN3vGb_"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "class TextGenerator():\n",
    "    \"\"\"\n",
    "        a class to preprocess a txt file availabe through the path\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path # a path to a txt file\n",
    "        self.text = \"\"\n",
    "        self.tokens_by_sentence = []\n",
    "        \n",
    "    def extractContent(self):\n",
    "        \"\"\"\n",
    "            loads the content of the file\n",
    "        \"\"\"\n",
    "        with open(self.path, 'r', encoding=\"utf-8\") as f: # open in readonly mode\n",
    "            # do your stuff\n",
    "            self.text = f.read()\n",
    "            \n",
    "    def tokenize_by_sentence(self, n=0):\n",
    "        \"\"\"\n",
    "            tokenizes a text into a list of tokens by sentence\n",
    "            inserts n times <start> and n times <end> of tags\n",
    "            at the beginning and the end of a sentence\n",
    "        \"\"\"\n",
    "        tmp = [self.sp_tags(self.normalize_text(sent), n) for sent in self.get_sentences()]\n",
    "        self.tokens_by_sentence = tmp\n",
    "    \n",
    "    def get_sentences(self) -> list:\n",
    "        \"\"\" uses nltk sent_tokenize to split the text into a list of sentences \"\"\"\n",
    "        return(sent_tokenize(self.text[0:250000])) # limit the coprus\n",
    "\n",
    "    def normalize_text(self, text) -> list:\n",
    "        \"\"\" normalizes text and turns it into a list of tokens \"\"\"\n",
    "        to_remove = punctuation + '«»“”'\n",
    "        tmp = [word.text.strip(to_remove) for word in razdel_tokenize(text)]\n",
    "        tmp = [word.lower() for word in tmp if word and len(word) < 20]\n",
    "        return(tmp)\n",
    "    \n",
    "    def sp_tags(self, tokens, n=0) -> list:\n",
    "        \"\"\" recieves a list of tokens and returns them with special tags \"\"\"\n",
    "        if n >= 0:\n",
    "            return(['<start>'] * n + tokens + ['<end>'] *n)\n",
    "        else:\n",
    "            return(ValueError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg3Jk6BqvGcA"
   },
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Jkh11LtlvGcA"
   },
   "outputs": [],
   "source": [
    "news = TextGenerator('data/lenta.txt')\n",
    "news.extractContent()\n",
    "# news.tokenize_by_sentence(2) # add 2 special tokens\n",
    "news.tokenize_by_sentence(0) # add no special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqKfK5YsvGcA"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "awHKTcyjvGcA"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class TextGenerator():\n",
    "    \"\"\"\n",
    "        a class to generate text based on a list tokens split by sentences\n",
    "        input: [['tokens', 'of', 'a', 'single', 'sentence']]\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens, ngram_length=2):\n",
    "        if ngram_length < 2: # for bigrams and large series \n",
    "            return(ValueError)\n",
    "        self.tokens = tokens\n",
    "        self.ngram_length = ngram_length\n",
    "        self.ngrams = [] # [0] (n-1)grams [1] ngrams\n",
    "        self.id2ngram = []\n",
    "        self.ngram2id = {}\n",
    "        self.matrix = np.empty(0)\n",
    "        \n",
    "     \n",
    "    def generate_ngrams(self):\n",
    "        \"\"\" generate ngrams from unigramgs \"\"\"\n",
    "        self.generate_counters() # generate enough counters for ngrams\n",
    "        \n",
    "        for sentence in self.tokens:\n",
    "            self.ngrams[0].update(\n",
    "                self.ngrammer(sentence, (self.ngram_length-1)) # gerenate ngrams of n-1 lenght\n",
    "                ) \n",
    "            self.ngrams[1].update(\n",
    "                self.ngrammer(sentence, self.ngram_length) # generate ngrams\n",
    "                )\n",
    "\n",
    "    def generate_counters(self):\n",
    "        \"\"\"\n",
    "            generates a number of ngrams Counter() to store in self.ngrams\n",
    "        \"\"\"\n",
    "        [self.ngrams.append(Counter()) for i in range(2)]\n",
    "        \n",
    "    def ngrammer(self, tokens, n=2) -> list:\n",
    "        \"\"\" return ngrams \"\"\"\n",
    "        ngrams = []\n",
    "        for i in range(0,len(tokens)-n+1):\n",
    "            ngrams.append(' '.join(tokens[i:i+n]))\n",
    "        return(ngrams)\n",
    "    \n",
    "    def build_matrix(self):\n",
    "        self.matrix = np.zeros(\n",
    "            (len(self.ngrams[0]), # rows with bigrams\n",
    "             len(self.ngrams[0]) # columns with unigrams\n",
    "            ))\n",
    "    \n",
    "        self.id2ngram = list(self.ngrams[0])\n",
    "        self.ngram2id = {ngram:i for i, ngram in enumerate(self.id2ngram)}\n",
    "        \n",
    "        \n",
    "        for ngram in self.ngrams[1]: # to re-write later to work on any ngram longer than 2\n",
    "            word1, word2, word3 = ngram.split()\n",
    "            bigram = word1 + \" \" + word2\n",
    "            bigram2 = word2 + \" \" + word3\n",
    "            self.matrix[self.ngram2id[bigram]][self.ngram2id[bigram2]] = (self.ngrams[1][ngram]/\n",
    "                                                                            self.ngrams[0][bigram])\n",
    "            \n",
    "    def generate_text(self, n=100, start='<start> <start>') -> str:\n",
    "        \"\"\" generates text with a calculated matrix \"\"\"\n",
    "        text = []\n",
    "        current_idx = self.ngram2id[start]\n",
    "        \n",
    "    \n",
    "        for i in range(n):\n",
    "\n",
    "            chosen = np.random.choice(self.matrix.shape[1], p=self.matrix[current_idx])\n",
    "            text.append(self.id2ngram[chosen].split()[1])\n",
    "\n",
    "            if self.id2ngram[chosen] == '<end> <end>':\n",
    "                chosen = self.ngram2id['<start> <start>']\n",
    "            current_idx = chosen\n",
    "\n",
    "        return '  '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SJCjLjkXvGcA"
   },
   "outputs": [],
   "source": [
    "news_generator = TextGenerator(news.tokens_by_sentence, 3)\n",
    "news_generator.generate_ngrams()\n",
    "news_generator.build_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuFqgchLx0xM",
    "outputId": "c4e336ad-0844-4d55-cd7c-0a6d78bac968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "основанием  для  ведения  в  стране  каждый  день  происходит  по  несколько  взрывов  разной  направленности  и  с  выкриками  севастополь  \n",
      "  \n",
      "  некурящие  рейсы  \n",
      "  \n",
      "  афанасьев  перечислил  потенциально  опасные  элементы  мира  на  солнце  чтобы  выдавать  необходимое  для  нормальной  работы  оставшихся  бортовых  систем  количество  электроэнергии  \n",
      "  \n",
      "  по  его  мнению  следует  лицензировать  деятельность  компаний  размещающих  информацию  в  интернете  содержащих  дезинформацию  о  событиях  на  северном  кавказе  \n",
      "  \n",
      "  тысячи  беженцев  спасаются  от  террора  в  южно-сахалинске  отмечена  вспышка  заболевания  холерой  \n",
      "  \n",
      "  кроме  того  сегодня  курс  достиг  рекордного  значения  за  всю  историю  утренних  торгов  и  составил  25,8702  руб  1  2  сентября  \n",
      "  \n",
      "  если  не\n"
     ]
    }
   ],
   "source": [
    "print(news_generator.generate_text().replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3wauLCO03bo",
    "outputId": "ddba85b4-f6ab-4b8a-9214-dfaea60bd523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "многие  дома  покрылись  трещинами  некоторые  опасно  накренились  \n",
      "  \n",
      "  такой  показ  событий  в  дагестане  отметил  собянин  \n",
      "  \n",
      "  об  этом  из  ошасо  ссылкой  на  пресс-центр  администрации  сахалинской  области  борис  дарижапов  в  реке  хомутовка  обнаружен  холерный  вибрион  класса  огава  \n",
      "  \n",
      "  минпечати  приняло  к  сведению  намерение  руководства  трк  пересмотреть  концепцию  и  стилистику  производства  пресловутой  передачи  top  1  в  москве  позавчера  прогремел  взрыв  в  торговом  комплексе  разрушения  незначительные  и  несущие  конструкции  не  пострадали  \n",
      "  \n",
      "  в  частности  системы  радиолокации  кабельного  телевидения  и  средства  мобильной  связи  частотой  выше  9  килогерц  \n",
      "  \n",
      "  в  некоторых  европейских  странах  саентологи  маскируются  под  экономические  и  общественные  организации\n"
     ]
    }
   ],
   "source": [
    "print(news_generator.generate_text().replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9SvFwOU048_",
    "outputId": "d06d36e3-8bb1-4e55-c91a-c84a38d10b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "среди  российских  военнослужащих  в  результате  проведенных  обысков  у  них  нет  никаких  свидетельств  тому  что  именно  иванов  предложил  послать  в  сша  российской  делегации  в  ходе  мероприятий  фсб  мвд  и  фсб  рост  цен  на  бензин  эти  проверки  стали  проводиться  более  активно  \n",
      "  \n",
      "  решение  министерства  может  быть  до  80  боевиков  \n",
      "  \n",
      "  милиция  блокировала  все  подъездные  пути  к  штаб-квартире  где  ведется  подсчет  голосов  начнется  в  среду  8  сентября  1999  года  ведущим  аналитической  программы  время  павлом  шереметом  по  поводу  интервью  георгия  бооса  независимой  газете  что  взрыв  на  манежной  площади  как  у  москвичей  появился  новый  повод  для  беспокойства  \n",
      "  \n",
      "  по  данным  на  20\n"
     ]
    }
   ],
   "source": [
    "print(news_generator.generate_text().replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvsH7_iC9BcB"
   },
   "source": [
    "# Task 2\n",
    "Задание 2. (5 баллов) Напишите функцию оценивания нграммов на основе PMI. Используйте эту функцию вместо дефолтной в gensim.models.Phrases Обучите два последовательных модели Phrases с такой функцией и проанализируйте результаты, получаемые после преобразования текстов двумя Phrases.\n",
    "\n",
    "Пояснения: Формулу PMI можно посмотреть вот тут https://en.wikipedia.org/wiki/Pointwise_mutual_information , также там описано как вывести вероятности из количества вхождений слова1, слова2, нграмма и размера корпуса. Чтобы функцию можно было поставить в аргумент scoring в Phrases у нее должны быть вот такие аргументы - scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count) Подберите параметр threshold под эту функцию. Чтобы проверить, что она вообще работает поставьте какое-то совсем маленькое число, чтобы порог проходили все нграммы и потом постепенно его повышайте. В тетрадке есть пример обучения нескольких Phrases последовательно, воспользуйтесь им."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9bxfkhLDvGcB",
    "outputId": "95135963-da76-4eeb-e83b-b161c91cb265"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "def scorer_pmi(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count):\n",
    "    \"\"\" \n",
    "        a function to evaluate ngrams to be used in gensim.Phraser\n",
    "        pmi formula is based on table retrieved from the following link\n",
    "        https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = np.log(\n",
    "            (bigram_count / worda_count) / \n",
    "            (wordb_count / corpus_word_count))\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uNMEsPHbvGcB",
    "outputId": "db6f025e-302a-475d-d631-91947c8bd944"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.034908170336502"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer_pmi(1938, 1311, 1159, 1000, 1000, 50000952)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дефолтная функция на низком пороге работает более устойчиво, не захватывая лишний мусор, созданная функция захватывает почти любые последовательсности\n",
    "\n",
    "На более высоком пороге, дефолтная функция также лучше справляется, позволяя устанавливать цепочки длиннее: 'со_ссылкой_на', 'в_том_числе'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_AtZMoIevGcB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['однако', 'уточненная', 'оценка', 'числа', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'достигнуть', 'ста', 'человек']\n",
      "['агентство', 'итар-тасс', 'в', 'сообщении', 'от', '21.15', 'со_ссылкой_на', 'источники', 'в', 'гувд', 'москвы', 'говорит', 'только', 'о', '30', 'раненых', 'в_том_числе', 'о', 'двух', 'пострадавших', 'в', 'тяжелом', 'состоянии']\n",
      "['однако', 'число', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'составить', 'до', 'ста', 'человек']\n",
      "['по_данным', 'риа_новости', 'боткинская', 'больница', 'институт', 'им']\n",
      "['склифосовского', '1-ая', 'градская', '36-ая', 'и', '64-ая', 'горбольница', 'работают', 'только', 'на', 'прием', 'пострадавших']\n"
     ]
    }
   ],
   "source": [
    "corpus = news.tokens_by_sentence[0:50000]\n",
    "# собираем статистики\n",
    "ph = gensim.models.Phrases(corpus, threshold = 15, scoring='default')\n",
    "# преобразовывать можно и через ph, но так быстрее \n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "# собираем статистики по уже забиграммленному тексту\n",
    "ph2 = gensim.models.Phrases(p[corpus])\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "for i in range(5):\n",
    "    print(p2[p[corpus[50+i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['однако', 'уточненная', 'оценка', 'числа', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'достигнуть', 'ста', 'человек']\n",
      "['агентство', 'итар-тасс', 'в', 'сообщении', 'от', '21.15', 'со_ссылкой', 'на', 'источники', 'в', 'гувд', 'москвы', 'говорит', 'только', 'о', '30', 'раненых', 'в_том', 'числе', 'о', 'двух', 'пострадавших', 'в', 'тяжелом', 'состоянии']\n",
      "['однако', 'число', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'составить', 'до', 'ста', 'человек']\n",
      "['по_данным', 'риа_новости', 'боткинская', 'больница', 'институт', 'им']\n",
      "['склифосовского', '1-ая', 'градская', '36-ая', 'и', '64-ая', 'горбольница', 'работают', 'только', 'на', 'прием', 'пострадавших']\n"
     ]
    }
   ],
   "source": [
    "corpus = news.tokens_by_sentence[0:50000]\n",
    "# собираем статистики\n",
    "ph = gensim.models.Phrases(corpus, threshold = 15, scoring=scorer_pmi)\n",
    "# преобразовывать можно и через ph, но так быстрее \n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "# собираем статистики по уже забиграммленному тексту\n",
    "ph2 = gensim.models.Phrases(p[corpus])\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "for i in range(5):\n",
    "    print(p2[p[corpus[50+i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gensim.models.phrases.original_scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2.scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KD_hw_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
