{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework on word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "1) Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно, и с помощью модели, взятой с rusvectores (например вот этой - http://vectors.nlpl.eu/repository/20/180.zip). Обучите 2 модели по определению перефразирования на получившихся векторах и проверьте, что работает лучше. \n",
    "\n",
    "Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие. \n",
    "!!!! ВАЖНО: Оценивать модели нужно с помощью кросс-валидации (в семинаре не кросс-валидация)! Метрика - f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normilizer():\n",
    "\n",
    "    def __init__(self, morph_type):\n",
    "        \n",
    "        self.morpho = MorphAnalyzer() \n",
    "        self.cashe = {}\n",
    "        self.stops = set(stopwords.words('russian'))\n",
    "        \n",
    "    \n",
    "    def normalize(self, text) -> list:\n",
    "        \"\"\"\n",
    "            returns a normalized text with POS tags mapped to udpipe\n",
    "        \"\"\"\n",
    "        \n",
    "        words = self.tokenize(text)\n",
    "        \n",
    "        res=[]\n",
    "        \n",
    "        mapping = self.generate_mapping('data/ru-rnc.map.txt')\n",
    "\n",
    "        for word in words:\n",
    "            if not word or word in self.stops:# skip stop words\n",
    "                continue \n",
    "            elif word in self.cashe: # check cashed first\n",
    "                res.append(self.cashe[word])\n",
    "            else:\n",
    "                r=self.morpho.parse(word)[0]\n",
    "                lemma = r.normal_form\n",
    "                pos = r.tag.POS\n",
    "                try:\n",
    "                    pos = mapping[pos]\n",
    "                    res.append(lemma+'_'+pos)\n",
    "                    self.cashe[word]=lemma+'_'+pos\n",
    "                except:\n",
    "                    res.append('Error')\n",
    "                \n",
    "        return res\n",
    "\n",
    "    \n",
    "    def tokenize(self, text) -> str:\n",
    "        \"\"\"\n",
    "            tokenizes a text and keeps only alphanumeric tokens\n",
    "        \"\"\"\n",
    "        punct = punctuation+'«»—…“”*№–'\n",
    "        \n",
    "        tokens = [token.text.strip(punct).lower() for token in list(razdel_tokenize(text))]\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def generate_mapping(self, path) -> dict:\n",
    "        \"\"\" \n",
    "            generates mapping of PoS tags to map mystem and udpipe tags:\n",
    "            Mapping was update for pymorphy2\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "\n",
    "        for line in open(path):\n",
    "            ms, ud = line.strip('\\n').split()\n",
    "            mapping[ms] = ud\n",
    "            \n",
    "        return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обучить_VERB',\n",
       " 'классификатор_NOUN',\n",
       " 'парафраз_NOUN',\n",
       " 'предобученный_ADJ',\n",
       " 'модель_NOUN',\n",
       " 'нужно_X',\n",
       " 'дом_NOUN']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('data/wiki_data.txt', encoding='utf8').read().splitlines()\n",
    "norm = Normilizer('pymorphy2')\n",
    "norm.normalize('Обучить классификатор парафразов на предобученной модели вам нужно будет дома')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_norm = [norm.normalize(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec([text for text in data_norm], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-05aad1014560>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  w2v.most_similar('полиция_NOUN')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('полицейский_ADJ', 0.8340840935707092),\n",
       " ('полицейский_NOUN', 0.8244892358779907),\n",
       " ('преступник_NOUN', 0.7986570596694946),\n",
       " ('милиция_NOUN', 0.7955402135848999),\n",
       " ('охранник_NOUN', 0.789502739906311),\n",
       " ('жандарм_NOUN', 0.786804735660553),\n",
       " ('подозревать_ADJ', 0.786204993724823),\n",
       " ('гестапо_NOUN', 0.7759683728218079),\n",
       " ('бандит_NOUN', 0.7743021845817566),\n",
       " ('спецназ_NOUN', 0.7564249634742737)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('полиция_NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1b166f3bf40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rusvectores http://vectors.nlpl.eu/repository/20/180.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusvec = gensim.models.KeyedVectors.load_word2vec_format('data/models/180/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('полиция_PROPN', 0.78785240650177),\n",
       " ('полицейский_ADJ', 0.7626974582672119),\n",
       " ('полицейский_NOUN', 0.6821430921554565),\n",
       " ('жандармерия_NOUN', 0.6472468376159668),\n",
       " ('жандарм_NOUN', 0.6468209624290466),\n",
       " ('городовый_ADJ', 0.6110862493515015),\n",
       " ('сыскный_ADJ', 0.6072753667831421),\n",
       " ('жандармский_ADJ', 0.6051692962646484),\n",
       " ('агент_NOUN', 0.5971013307571411),\n",
       " ('градоначальник_NOUN', 0.5876316428184509)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rusvec.most_similar('полиция_NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    \"\"\" word2vec vectorizer \"\"\"\n",
    "    \n",
    "    def __init__(self, model, dim):\n",
    "        self.model = model # Gensim w2v model\n",
    "        self.dim = dim\n",
    "        \n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\" transforms a text into a vector using w2v model \"\"\"\n",
    "        \n",
    "        text = text.split()\n",
    "        words = Counter(text) # cashe words\n",
    "        total = len(text)\n",
    "        vectors = np.zeros((len(words), self.dim))\n",
    "\n",
    "        for i,word in enumerate(words):\n",
    "            try:\n",
    "                v = self.model[word]\n",
    "                vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "            except (KeyError, ValueError):\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        if vectors.any():\n",
    "            vector = np.average(vectors, axis=0)\n",
    "        else:\n",
    "            vector = np.zeros((self.dim))\n",
    "        \n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('data/paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "\n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(norm.normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(norm.normalize)\n",
    "data['text_1_norm'] = data['text_1_norm'].apply(\" \".join)\n",
    "data['text_2_norm'] = data['text_2_norm'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0</td>\n",
       "      <td>полицейский_NOUN разрешить_VERB стрелять_VERB ...</td>\n",
       "      <td>полиция_NOUN мочь_VERB разрешить_VERB стрелять...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0</td>\n",
       "      <td>право_NOUN полицейский_ADJ проникновение_NOUN ...</td>\n",
       "      <td>правило_NOUN внесудебный_ADJ проникновение_NOU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  Право полицейских на проникновение в жилище ре...   \n",
       "\n",
       "                                              text_2 label  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...     0   \n",
       "1  Правила внесудебного проникновения полицейских...     0   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский_NOUN разрешить_VERB стрелять_VERB ...   \n",
       "1  право_NOUN полицейский_ADJ проникновение_NOUN ...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  полиция_NOUN мочь_VERB разрешить_VERB стрелять...  \n",
       "1  правило_NOUN внесудебный_ADJ проникновение_NOU...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2957\n",
       "-1    2582\n",
       "1     1688\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['label']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-75724c73e890>:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = self.model[word]\n"
     ]
    }
   ],
   "source": [
    "dim=50\n",
    "\n",
    "W2V = Vectorizer(w2v, dim)\n",
    "\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = W2V.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = W2V.get_embedding(text)\n",
    "    \n",
    "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02467497, -0.0424685 ,  0.00706543,  0.07053129,  0.02111456,\n",
       "        0.00026692,  0.02947436, -0.00262682, -0.02419937, -0.03342579,\n",
       "        0.06407263,  0.03926686,  0.00204358, -0.03341437, -0.01235045,\n",
       "       -0.02330916,  0.04435799, -0.00258285,  0.06149818,  0.02719655,\n",
       "       -0.01207639, -0.00268261, -0.05692433, -0.02179623,  0.01853627,\n",
       "       -0.09280529,  0.06385348, -0.03415607, -0.03120847, -0.04610864,\n",
       "        0.02290655,  0.06502387,  0.00248277,  0.01359456,  0.00874802,\n",
       "       -0.04827983, -0.06036325,  0.02155151,  0.07595971, -0.01363526,\n",
       "        0.04501745, -0.00981721,  0.0408394 , -0.00950802, -0.08377997,\n",
       "       -0.06442852, -0.02202226,  0.00171277,  0.0041399 , -0.01984598,\n",
       "       -0.01384089, -0.07207043, -0.01958374,  0.06150077,  0.02150663,\n",
       "        0.01519106,  0.02775176, -0.00300476, -0.0246584 , -0.05242948,\n",
       "        0.05648939,  0.03133879,  0.02379792, -0.01279469, -0.00908745,\n",
       "       -0.02772011,  0.04417225,  0.00640621,  0.04720146,  0.0491051 ,\n",
       "       -0.03289265,  0.0275723 , -0.03275978, -0.02367326,  0.01778768,\n",
       "       -0.09701724,  0.05816478, -0.0492936 , -0.02003234, -0.05638205,\n",
       "        0.03386615,  0.06721927,  0.02237046, -0.01102611, -0.01029221,\n",
       "       -0.04767725, -0.052084  ,  0.02466159,  0.05883339, -0.0098921 ,\n",
       "        0.02494277,  0.01445485,  0.02356422, -0.01042215, -0.06432092,\n",
       "       -0.05191208,  0.01376947, -0.01699152,  0.01303816,  0.00161924])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=50\n",
    "\n",
    "RV = Vectorizer(rusvec, dim)\n",
    "\n",
    "X_text_1_rv = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_rv = np.zeros((len(data['text_2_norm']), dim))\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_rv[i] = RV.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_rv[i] = RV.get_embedding(text)\n",
    "    \n",
    "X_text_rv = np.concatenate([X_text_1_rv, X_text_2_rv], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_rv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_rv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf2 = LogisticRegression(C=10000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4571231  0.48547718 0.49065744 0.35709343 0.38961938]\n",
      "[0.38589212 0.41355463 0.42698962 0.35363322 0.3349481 ]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf1, X_text_w2v, y, scoring=\"f1_micro\"))\n",
    "print(cross_val_score(clf2, X_text_w2v, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35684647 0.35684647 0.23321799 0.40899654 0.23391003]\n",
      "[0.35684647 0.35684647 0.35778547 0.35778547 0.35709343]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf1, X_text_rv, y, scoring=\"f1_micro\"))\n",
    "print(cross_val_score(clf2, X_text_rv, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары).\n",
    "\n",
    "Постройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру).  Попробуйте улучить метрику, изменив параметры в методах векторизации.\n",
    "!!УТОЧНЕНИЕ: модель нужно обучить сразу на всех 5 близостях, а не по 1 модели на каждой близости!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 vector types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.4, max_features=1000, min_df=3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "tfidf.fit(pd.concat([data['text_1_norm'], data['text_2_norm']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(200)\n",
    "\n",
    "X_text_1_svd = svd.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_svd = svd.fit_transform(tfidf.transform(data['text_2_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(100)\n",
    "\n",
    "X_text_1_nmf = nmf.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_nmf = nmf.fit_transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_nmf = np.concatenate([X_text_1_nmf, X_text_2_nmf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d1a9c2abcf3c>:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = self.model[word]\n"
     ]
    }
   ],
   "source": [
    "w2v = gensim.models.Word2Vec([text for text in data_norm], size=50, sg=1)\n",
    "\n",
    "dim=50\n",
    "\n",
    "W2V = Vectorizer(w2v, dim)\n",
    "\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = W2V.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = W2V.get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d1a9c2abcf3c>:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = self.model[word]\n"
     ]
    }
   ],
   "source": [
    "fast_text = gensim.models.FastText([text for text in data_norm], size=50, min_n=4, max_n=8) \n",
    "\n",
    "dim=50\n",
    "\n",
    "FT = Vectorizer(fast_text, dim)\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = FT.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = FT.get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RV = Vectorizer(rusvec, dim)\n",
    "\n",
    "X_text_1_rv = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_rv = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_rv[i] = RV.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_rv[i] = RV.get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07642255]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X_text_1_svd[0:1], X_text_2_svd[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_row_similarity(A, B):\n",
    "    \"\"\" calculates similarity between each row of matrix A and matrix B \"\"\"\n",
    "    output = []\n",
    "    for row in zip(A, B):\n",
    "        # add a value only\n",
    "        output.extend(cosine_similarity([row[0]], [row[1]])[0])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd</th>\n",
       "      <th>nmf</th>\n",
       "      <th>w2v</th>\n",
       "      <th>ft</th>\n",
       "      <th>rv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076423</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.914622</td>\n",
       "      <td>0.897895</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.231177</td>\n",
       "      <td>0.597571</td>\n",
       "      <td>0.930156</td>\n",
       "      <td>0.865366</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.098049</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.950981</td>\n",
       "      <td>0.792569</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.206381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.792817</td>\n",
       "      <td>0.575355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211944</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.924721</td>\n",
       "      <td>0.684357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        svd       nmf       w2v        ft   rv\n",
       "0  0.076423  0.000123  0.914622  0.897895  0.0\n",
       "1  0.231177  0.597571  0.930156  0.865366  0.0\n",
       "2 -0.098049  0.044629  0.950981  0.792569  0.0\n",
       "3 -0.206381  0.000000  0.792817  0.575355  0.0\n",
       "4 -0.211944  0.000741  0.924721  0.684357  0.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_sim = calculate_row_similarity(X_text_1_svd, X_text_2_svd)\n",
    "nmf_sim = calculate_row_similarity(X_text_1_nmf, X_text_2_nmf)\n",
    "w2v_sim = calculate_row_similarity(X_text_1_w2v, X_text_2_w2v)\n",
    "ft_sim = calculate_row_similarity(X_text_1_ft, X_text_2_ft)\n",
    "rv_sim = calculate_row_similarity(X_text_1_rv, X_text_2_rv)\n",
    "\n",
    "similarities = pd.DataFrame({\n",
    "    'svd': svd_sim,\n",
    "    \"nmf\": nmf_sim,\n",
    "    'w2v': w2v_sim,\n",
    "    'ft' : ft_sim,\n",
    "    'rv' : rv_sim,\n",
    "})\n",
    "\n",
    "similarities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификаторы 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf2 = LogisticRegression(C=10000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5373444  0.56431535 0.61453287 0.46228374 0.44844291]\n",
      "[0.51452282 0.53526971 0.58062284 0.42698962 0.43598616]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf1, similarities, y, scoring=\"f1_micro\"))\n",
    "print(cross_val_score(clf2, similarities, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификаторы 2: Изменение гиперпараметров (увеличение или уменьшение) не дает ощутимой разницы в результатах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_leaf=10,\n",
    "                             class_weight='balanced')\n",
    "clf2 = LogisticRegression(C=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54149378 0.56639004 0.61591696 0.45743945 0.44636678]\n",
      "[0.51452282 0.53526971 0.58062284 0.42629758 0.43598616]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf1, similarities, y, scoring=\"f1_micro\"))\n",
    "print(cross_val_score(clf2, similarities, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оба классификатора, обученные на косинусной близости, дают более высокий результат в сравнении с классификаторами обученными только на эмбеддингах ~ +0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Изменение методов векторизации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.7, max_features=2000, min_df=5)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, max_df=0.7, max_features=2000)\n",
    "tfidf.fit(pd.concat([data['text_1_norm'], data['text_2_norm']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(300)\n",
    "\n",
    "X_text_1_svd = svd.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_svd = svd.fit_transform(tfidf.transform(data['text_2_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(150)\n",
    "\n",
    "X_text_1_nmf = nmf.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_nmf = nmf.fit_transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_nmf = np.concatenate([X_text_1_nmf, X_text_2_nmf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d1a9c2abcf3c>:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = self.model[word]\n"
     ]
    }
   ],
   "source": [
    "w2v = gensim.models.Word2Vec([text for text in data_norm], size=100, sg=1)\n",
    "\n",
    "dim=100\n",
    "\n",
    "W2V = Vectorizer(w2v, dim)\n",
    "\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = W2V.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = W2V.get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d1a9c2abcf3c>:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = self.model[word]\n"
     ]
    }
   ],
   "source": [
    "fast_text = gensim.models.FastText([text for text in data_norm], size=100, min_n=4, max_n=8) \n",
    "\n",
    "dim=100\n",
    "\n",
    "FT = Vectorizer(fast_text, dim)\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = FT.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = FT.get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "RV = Vectorizer(rusvec, 100)\n",
    "\n",
    "X_text_1_rv = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_rv = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_rv[i] = RV.get_embedding(text)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_rv[i] = RV.get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd</th>\n",
       "      <th>nmf</th>\n",
       "      <th>w2v</th>\n",
       "      <th>ft</th>\n",
       "      <th>rv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.067069</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.874152</td>\n",
       "      <td>0.890109</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009539</td>\n",
       "      <td>0.030172</td>\n",
       "      <td>0.898566</td>\n",
       "      <td>0.869398</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.012295</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.919779</td>\n",
       "      <td>0.773450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.106305</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.783218</td>\n",
       "      <td>0.574517</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073217</td>\n",
       "      <td>0.080916</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.658687</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        svd       nmf       w2v        ft   rv\n",
       "0 -0.067069  0.000165  0.874152  0.890109  0.0\n",
       "1  0.009539  0.030172  0.898566  0.869398  0.0\n",
       "2 -0.012295  0.002107  0.919779  0.773450  0.0\n",
       "3 -0.106305  0.000304  0.783218  0.574517  0.0\n",
       "4 -0.073217  0.080916  0.904459  0.658687  0.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_sim = calculate_row_similarity(X_text_1_svd, X_text_2_svd)\n",
    "nmf_sim = calculate_row_similarity(X_text_1_nmf, X_text_2_nmf)\n",
    "w2v_sim = calculate_row_similarity(X_text_1_w2v, X_text_2_w2v)\n",
    "ft_sim = calculate_row_similarity(X_text_1_ft, X_text_2_ft)\n",
    "rv_sim = calculate_row_similarity(X_text_1_rv, X_text_2_rv)\n",
    "\n",
    "similarities = pd.DataFrame({\n",
    "    'svd': svd_sim,\n",
    "    \"nmf\": nmf_sim,\n",
    "    'w2v': w2v_sim,\n",
    "    'ft' : ft_sim,\n",
    "    'rv' : rv_sim,\n",
    "})\n",
    "\n",
    "similarities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сравннение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf2 = LogisticRegression(C=10000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52697095 0.56984786 0.60415225 0.47543253 0.4615917 ]\n",
      "[0.5055325  0.54771784 0.5799308  0.43183391 0.43252595]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf1, similarities, y, scoring=\"f1_micro\"))\n",
    "print(cross_val_score(clf2, similarities, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_leaf=10,\n",
    "                             class_weight='balanced')\n",
    "clf2 = LogisticRegression(C=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52766252 0.56777317 0.60276817 0.46989619 0.45051903]\n",
      "[0.5055325  0.54771784 0.5799308  0.43183391 0.43252595]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf1, similarities, y, scoring=\"f1_micro\"))\n",
    "print(cross_val_score(clf2, similarities, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение параметров не дает заметной разницы (+-0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
