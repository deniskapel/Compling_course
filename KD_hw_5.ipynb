{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Реализуйте простую версию byte pair encoding. Алгоритм должен работать так:\n",
    "строки с текстом разбиваются на отдельные символы и далее в цикле из N итераций: а) считаются статистики встречаемости по парам символов и б) топ-K частотных пар склеиваются в один символ\n",
    "\n",
    "Попробуйте так токенизировать текст с разными параметрами N и K. Проанализируйте словарь уникальных слов для нескольких наборов параметров - сколько уникальных слов получилось, какой токен самый длинный?\n",
    "\n",
    "(5 баллов)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from string import punctuation\n",
    "\n",
    "class BPEtokenizer():\n",
    "    \"\"\"\n",
    "        a class to tokenize a dataset using BPE\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path # a path to a csv file\n",
    "        self.text = \"\"\n",
    "        \n",
    "    def load_text(self):\n",
    "        \"\"\" loads the content of the txt file \"\"\"\n",
    "        with open(self.path, 'r', encoding=\"utf-8\") as f: # open in readonly mode\n",
    "            self.text = f.read()\n",
    "            \n",
    "    def split_to_chars(self, text) -> list:\n",
    "        \"\"\" split texts into single chars\"\"\"\n",
    "        return(list(text.lower()))\n",
    "    \n",
    "    \n",
    "    def merge_chars(self, chars, N=1, K=10) -> list:\n",
    "        \"\"\" \n",
    "            use ngrams\n",
    "                to identify K-top neigbouring elements\n",
    "                then merge them\n",
    "                repeat the cycle N\n",
    "                Maximum ngram length is 2^N \n",
    "        \"\"\"\n",
    "        tmp = chars\n",
    "        for _ in range(N):\n",
    "            tmp = self.get_common_neighbours(tmp, K)\n",
    "            \n",
    "        return tmp\n",
    "    \n",
    "    def get_common_neighbours(self, chars, K=10) -> list:\n",
    "        \"\"\"\n",
    "            iterate over a list of char and choose Top K neibours\n",
    "        \"\"\"\n",
    "        pairs = [frst+scnd for frst, scnd in zip(chars[:-1], chars[1:])]\n",
    "        \n",
    "        filtered = self.filter_neighbours(Counter(pairs), K)\n",
    "        \n",
    "        output = [chars[0]]\n",
    "        \n",
    "        for char in chars[1:]:\n",
    "            pair = output[-1] + char\n",
    "            \n",
    "            if pair in filtered:\n",
    "                output[-1] = pair\n",
    "            else:\n",
    "                output.append(char)\n",
    "                \n",
    "        return output\n",
    "            \n",
    "    def filter_neighbours(self, counter, K=10) -> list:\n",
    "        \"\"\" \n",
    "            filter collections.Counter to top K groups\n",
    "            with > 2 occurances \n",
    "        \"\"\"\n",
    "        return [word for word, count in counter.most_common(K) if count > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Statistics():\n",
    "        \"\"\"\n",
    "        a class to get basic statistics from a tokenized text\n",
    "        \"\"\"\n",
    "        def __init__(self, tokens):\n",
    "            self.tokens = tokens \n",
    "\n",
    "        def counter(self):\n",
    "            \"\"\" calculates uniqueloads the content of the txt file \"\"\"\n",
    "            return Counter(self.tokens)\n",
    "        \n",
    "        def count_unique(self) -> int:\n",
    "            \"\"\" count unique tokens \"\"\"\n",
    "            return len(set(self.tokens))\n",
    "        \n",
    "        def get_longest(self, length):\n",
    "            \"\"\" get and return the length of vocavulary entries longer than a given value \"\"\"\n",
    "            mapping = defaultdict(int)\n",
    "            \n",
    "            for word in set(self.tokens):\n",
    "                mapping[word] += len(word)\n",
    "                \n",
    "            return {k: v for k,v in mapping.items() if v>length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = BPEtokenizer('data/lenta.txt')\n",
    "news.load_text()\n",
    "chars = news.split_to_chars(news.text[:1000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = news.merge_chars(chars, 3, 500)\n",
    "stat = Statistics(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' государ': 8,\n",
       " ' российс': 8,\n",
       " ' компани': 8,\n",
       " 'правител': 8,\n",
       " 'министра': 8,\n",
       " 'вительст': 8,\n",
       " ', которы': 8,\n",
       " ' человек': 8,\n",
       " ' по слов': 8,\n",
       " 'сегодня ': 8,\n",
       " 'представ': 8,\n",
       " 'российск': 8,\n",
       " ' сообщил': 8,\n",
       " ' правите': 8,\n",
       " ' \"новост': 8,\n",
       " 'ительств': 8,\n",
       " ' россии ': 8,\n",
       " ' предста': 8,\n",
       " 'правлени': 8,\n",
       " ' министр': 8,\n",
       " 'компании': 8,\n",
       " ' президе': 8,\n",
       " ' как соо': 8,\n",
       " ' сегодня': 8,\n",
       " '. как со': 8,\n",
       " 'президен': 8}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.get_longest(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1574"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.count_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = news.merge_chars(chars, 4, 100)\n",
    "stat = Statistics(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'как сообщ': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.get_longest(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.count_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = news.merge_chars(chars, 2, 300)\n",
    "stat = Statistics(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'дент': 4,\n",
       " 'лова': 4,\n",
       " 'каза': 4,\n",
       " ' рос': 4,\n",
       " 'ьств': 4,\n",
       " 'лись': 4,\n",
       " '. ка': 4,\n",
       " 'ских': 4,\n",
       " 'к со': 4,\n",
       " ' пол': 4,\n",
       " 'стан': 4,\n",
       " 'теле': 4,\n",
       " 'ных ': 4,\n",
       " 'ный ': 4,\n",
       " 'сего': 4,\n",
       " 'общи': 4,\n",
       " 'ства': 4,\n",
       " 'росс': 4,\n",
       " 'маци': 4,\n",
       " 'родн': 4,\n",
       " 'мент': 4,\n",
       " 'орма': 4,\n",
       " 'жени': 4,\n",
       " ' соо': 4,\n",
       " 'редс': 4,\n",
       " 'заяв': 4,\n",
       " 'ать ': 4,\n",
       " ', чт': 4,\n",
       " 'авит': 4,\n",
       " 'рабо': 4,\n",
       " 'ског': 4,\n",
       " 'я на': 4,\n",
       " ' буд': 4,\n",
       " 'врем': 4,\n",
       " 'шени': 4,\n",
       " 'ерал': 4,\n",
       " 'ния ': 4,\n",
       " 'на с': 4,\n",
       " 'моск': 4,\n",
       " 'ний ': 4,\n",
       " 'ся в': 4,\n",
       " 'нные': 4,\n",
       " 'ном ': 4,\n",
       " 'пост': 4,\n",
       " ', ко': 4,\n",
       " 'ител': 4,\n",
       " 'ции ': 4,\n",
       " 'том ': 4,\n",
       " 'прав': 4,\n",
       " '. пр': 4,\n",
       " 'нени': 4,\n",
       " 'льно': 4,\n",
       " 'анны': 4,\n",
       " 'на п': 4,\n",
       " 'рова': 4,\n",
       " '. в ': 4,\n",
       " 'альн': 4,\n",
       " ' его': 4,\n",
       " ' сво': 4,\n",
       " ' это': 4,\n",
       " 'что ': 4,\n",
       " 'иков': 4,\n",
       " 'или ': 4,\n",
       " 'того': 4,\n",
       " 'ости': 4,\n",
       " 'ких ': 4,\n",
       " 'венн': 4,\n",
       " 'цион': 4,\n",
       " 'орга': 4,\n",
       " 'и в ': 4,\n",
       " ' в п': 4,\n",
       " 'тран': 4,\n",
       " 'дет ': 4,\n",
       " 'и и ': 4,\n",
       " 'о пр': 4,\n",
       " 'твен': 4,\n",
       " 'иров': 4,\n",
       " 'част': 4,\n",
       " 'нию ': 4,\n",
       " ' бол': 4,\n",
       " 'иден': 4,\n",
       " 'вани': 4,\n",
       " ' год': 4,\n",
       " '. по': 4,\n",
       " ', на': 4,\n",
       " 'кого': 4,\n",
       " 'мини': 4,\n",
       " 'раци': 4,\n",
       " ' при': 4,\n",
       " ' как': 4,\n",
       " 'прин': 4,\n",
       " 'тся ': 4,\n",
       " 'его ': 4,\n",
       " ' кон': 4,\n",
       " 'наро': 4,\n",
       " ' из ': 4,\n",
       " ' раз': 4,\n",
       " 'ные ': 4,\n",
       " ' под': 4,\n",
       " 'я по': 4,\n",
       " 'банк': 4,\n",
       " ' пра': 4,\n",
       " 'дани': 4,\n",
       " ' так': 4,\n",
       " 'след': 4,\n",
       " 'пред': 4,\n",
       " 'рани': 4,\n",
       " ' рас': 4,\n",
       " 'оста': 4,\n",
       " 'о в ': 4,\n",
       " 'ийск': 4,\n",
       " 'ает ': 4,\n",
       " 'щест': 4,\n",
       " 'ения': 4,\n",
       " ' в с': 4,\n",
       " 'ообщ': 4,\n",
       " ', пр': 4,\n",
       " 'прос': 4,\n",
       " 'можн': 4,\n",
       " 'я в ': 4,\n",
       " '. на': 4,\n",
       " 'мест': 4,\n",
       " 'ации': 4,\n",
       " 'пере': 4,\n",
       " 'ност': 4,\n",
       " 'ован': 4,\n",
       " ', со': 4,\n",
       " 'посл': 4,\n",
       " 'ческ': 4,\n",
       " 'езид': 4,\n",
       " 'ельн': 4,\n",
       " ' пре': 4,\n",
       " 'годн': 4,\n",
       " 'сооб': 4,\n",
       " 'и пр': 4,\n",
       " 'отор': 4,\n",
       " 'торо': 4,\n",
       " 'ство': 4,\n",
       " 'ател': 4,\n",
       " 'нист': 4,\n",
       " 'осси': 4,\n",
       " 'рори': 4,\n",
       " 'дени': 4,\n",
       " 'тво ': 4,\n",
       " 'авле': 4,\n",
       " 'вать': 4,\n",
       " 'ого ': 4,\n",
       " ', в ': 4,\n",
       " 'стви': 4,\n",
       " ' тер': 4,\n",
       " ' за ': 4,\n",
       " 'пани': 4,\n",
       " 'нных': 4,\n",
       " 'тва ': 4,\n",
       " 'ется': 4,\n",
       " 'дств': 4,\n",
       " ', по': 4,\n",
       " 'став': 4,\n",
       " ' ком': 4,\n",
       " 'едст': 4,\n",
       " 'ным ': 4,\n",
       " 'о по': 4,\n",
       " 'през': 4,\n",
       " ' что': 4,\n",
       " ' дол': 4,\n",
       " 'кото': 4,\n",
       " 'ного': 4,\n",
       " ' воз': 4,\n",
       " 'вите': 4,\n",
       " 'нии ': 4,\n",
       " 'ние ': 4,\n",
       " ' был': 4,\n",
       " 'бря ': 4,\n",
       " ' сло': 4,\n",
       " 'нско': 4,\n",
       " 'по с': 4,\n",
       " ' мил': 4,\n",
       " 'сти ': 4,\n",
       " 'деле': 4,\n",
       " 'го с': 4,\n",
       " 'ить ': 4,\n",
       " 'и по': 4,\n",
       " 'ной ': 4,\n",
       " ' не ': 4,\n",
       " 'едер': 4,\n",
       " ' стр': 4,\n",
       " ' про': 4,\n",
       " 'сии ': 4,\n",
       " 'ступ': 4,\n",
       " 'йств': 4,\n",
       " ' по ': 4,\n",
       " 'ов, ': 4,\n",
       " ' в р': 4,\n",
       " 'вост': 4,\n",
       " 'ссии': 4,\n",
       " 'ельс': 4,\n",
       " 'стве': 4,\n",
       " 'енно': 4,\n",
       " 'пров': 4,\n",
       " ' от ': 4,\n",
       " 'как ': 4,\n",
       " 'ичес': 4,\n",
       " 'е по': 4,\n",
       " 'ами ': 4,\n",
       " 'ссий': 4,\n",
       " ' все': 4,\n",
       " ' ста': 4,\n",
       " 'стра': 4,\n",
       " 'ново': 4,\n",
       " ' пос': 4,\n",
       " 'я пр': 4,\n",
       " 'жден': 4,\n",
       " 'льны': 4,\n",
       " 'явил': 4,\n",
       " 'е пр': 4,\n",
       " 'ться': 4,\n",
       " 'чени': 4,\n",
       " 'нием': 4,\n",
       " 'ской': 4,\n",
       " 'цент': 4,\n",
       " 'тов ': 4,\n",
       " 'циал': 4,\n",
       " 'влен': 4,\n",
       " 'ков ': 4,\n",
       " 'ение': 4,\n",
       " 'стно': 4,\n",
       " 'щени': 4,\n",
       " 'а пр': 4,\n",
       " 'тори': 4,\n",
       " 'кой ': 4,\n",
       " 'ходи': 4,\n",
       " ' пер': 4,\n",
       " 'торы': 4,\n",
       " ', а ': 4,\n",
       " 'рист': 4,\n",
       " 'лени': 4,\n",
       " 'тель': 4,\n",
       " 'енны': 4,\n",
       " 'льст': 4,\n",
       " 'осто': 4,\n",
       " ' мос': 4,\n",
       " ' кот': 4,\n",
       " ' для': 4,\n",
       " ' на ': 4,\n",
       " 'ость': 4,\n",
       " 'для ': 4,\n",
       " ' мин': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.get_longest(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.count_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 extra\n",
    "Чтобы получить 1 бонусный балл - зафиксируйте получившийся словарь и токенизируйте с помощью него текст, который ранее не встречался в корпусе (возьмите рандомную новость из яндекс новостей например). Проанализируйте насколько хорошо токенизировался текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_dict = set(tmp)\n",
    "test_corpus = open('data/nplus1.txt', 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1,4), vocabulary=static_dict)\n",
    "X = vectorizer.fit_transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' буд', ' был', ' в', ' в п', ' в р', ' в с', ' воз', ' все', ' вы', ' г']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 100   0   0   0   1   1   3   1   0   0   1   1   0   0   0   0   0\n",
      "   0   1   0   7   5   1   0   0   0   1   1   4   0   0   4   0   0   0\n",
      "   1   0   0   0   0   0   2   0   1   0   4   1   1   0   3   1   0   0\n",
      "   1   0   0   1   0   0   4   1   1   0   6   0   0   1   0   0   0   0\n",
      "   0   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   9   3   0   0   0   0   0   0   0   0   0   0   0   2   2   0\n",
      "   0   0   0   0   0   0   4   0   0   0   1   1   1   0   2   1   7   2\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  26   7   0   0   0   0   0   4   0   0   0   1   2   0   0   1   0\n",
      "   0   0   0   0   0   1   0   0   2   2   3   0   0   0   0   0   1   1\n",
      "   1   3   0   0   0   0   1   0   1   0   0   0   1  17   3   1   0   3\n",
      "   1   0   2   1   0   0   1   0   1   0   2   1   0   0   0   0   0   0\n",
      "   2   1   0   0   0   1   0   0   0   0  12   1   0   0   0   0   0   0\n",
      "   0   0   0   0   5   5   3   2   1   1   0   0   0  20   1   0   0   0\n",
      "   0   1   0   0   1   0   0   0   0   2   0   2   0   0   0   0   2   3\n",
      "   1   0   1   0   0   1   0   1   4   1   1   0   0   0   0   0   0   0\n",
      "   0   5   0   1   0   0   0   2   0   0  13   3   0   0   0   0   0   0\n",
      "   0   1   0   0   2   0   0   0   0   1   1   2   0   2   0   0   0   0\n",
      "   0   0   0   0   1   0   0   0   0   1   2   1   0   0   0   0   7   0\n",
      "   0   1   0   0   0   1   0   0   4   1   0   0   0   1   0   0   0  20\n",
      "   1   0   5   0   0   0   0   1   1   0   3   3   0   0   0   0   6   7\n",
      "   3   0   0   0   2   0   0   0   0   0   0   0   0   0  18   0   2   0\n",
      "   0   0   0   1   0   5   0   1   0   0   0   0   1   1   0   1   0   0\n",
      "   4   0   0   0   0   0   0   0   0   0   0   0   5   0   0   1   3   0\n",
      "  28   1   0   0   0   0   2   0   0   5   1   0   0   0   3   0   0   1\n",
      "   0   0   0   3   1   1   0   1   0   0   4   0   0   3   0   2   0   0\n",
      "   0   3   1   0   0   6   0   0   1   1   0   0   2   0   0   1   0   0\n",
      "   1   0   0   0   0   0   0   0   1  16   1   5   0   1   0   0   0   4\n",
      "   0   0   0   0   4   0   2   0   0   0   0   0   1   0  15   0   0   0\n",
      "   1   0   0   0   0   0   0   0   1   0   2   2   0   2   0   0   0   0\n",
      "   6   1   0   1   1   1   0   0   0   0   0   0   0   0   0   1   0  23\n",
      "   2   1   4   0   4   1   1   0   1   0   0   0   0   2   3   0   0   0\n",
      "   0   0   1   1   0   3   1   0   0   1   0   0   7   0   0   0   0   0\n",
      "   3   2   0   0   0   1   3   0   0   0   0   8   5   0   0   0   2   2\n",
      "   0   0   0   0   0   6   2   0   1   0   0   0   1   0   1   1   1   0\n",
      "   1   1   0   0   0   0   0   0   0   0   9   1   0   0   0   0   1   1\n",
      "   0   6   4   1   0   0   0   0   0   1   0   1   0  10   8   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray()[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Обучите токенизатор из tokenizers на текстовом корпусе. Рассчитайте статистики для idf по корпусу, используя обученный словарь (разбейте корпус на \"документы\" по новым строкам, каждый \"документ\" токенизируйте, для каждого слова посчитайте, в скольких документах оно встречается и рассчитайте idf разделив общее количество документов на это число, возьмите логарифм от полученного числа). \n",
    "Векторизуйте текст (в мешок слов) аналогично TfidfVectorizer, используя токенизатор и idf статистики (инициализируйте*** пустую матрицу размером (N документов, K слов в словаре) и в цикле по всем документам постепенно заполните ее - токенизируйте документ, рассчитайте TF каждого слова (количество вхождений в документе поделить на общее количество слов в документе), умножьте TF на IDF и, используя индексы слов в словаре, запишите получившееся значение в матрицу)\n",
    "\n",
    "Формулу для TFIDF можете уточнить тут -  https://ru.wikipedia.org/wiki/TF-IDF\n",
    "\n",
    "***Чтобы инициализировать разреженную матрицу используйте scipy:\n",
    "from scipy.sparse import lil_matrix\n",
    "X = lil_matrix(N, K)\n",
    "\n",
    "Обучите классификатор на полученных векторах и оцените на кросс-валидации. \n",
    "\n",
    "(5 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer, Tokenizer\n",
    "from scipy.sparse import lil_matrix\n",
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor():\n",
    "    \"\"\"\n",
    "        a class to process a txt file\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path # a path to a csv file\n",
    "        self.df = pd.DataFrame()\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        \"\"\" loads the content of the csv file \"\"\"\n",
    "        self.df = pd.read_csv(self.path)\n",
    "\n",
    "data = TextProcessor('data/labeled.csv')\n",
    "data.load_dataset()\n",
    "data.df.head()\n",
    "data.df['comment'].to_csv('data/bpe_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sub = CharBPETokenizer()\n",
    "tok_sub.train('data/bpe_corpus.txt', vocab_size=2000, min_frequency=10,)\n",
    "tok_sub.save('data/2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sub = Tokenizer.from_file(\"data/2k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myBPEvectorizer():\n",
    "    \n",
    "    \"\"\"\n",
    "        a class to vectorize a corpus using\n",
    "        a prepared CharBPETokenizer() model\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.tokenizer = Tokenizer.from_file(path) # a path to a csv file\n",
    "        self.tokenized = []\n",
    "        self.tfidf = []\n",
    "        \n",
    "    def tokenize(self, corpus):\n",
    "        \"\"\" tokenize a corpus of texts using a pre-trained model \"\"\"\n",
    "        self.tokenized = [self.tokenizer.encode(doc) for doc in corpus]\n",
    "        \n",
    "    def get_ids(self) -> list:\n",
    "        \"\"\" retrieve ids of tokens for each text \"\"\"\n",
    "        return [doc.ids for doc in self.tokenized]\n",
    "    \n",
    "    def get_tokens(self) -> list:\n",
    "        \"\"\" retrieve ids of tokens for each text \"\"\"\n",
    "        return [doc.tokens for doc in self.tokenized]\n",
    "    \n",
    "    def initiate_matrix(self, N):\n",
    "        \"\"\" use scipy to initiate a sparce matrix \"\"\"\n",
    "        self.tfidf = lil_matrix((N,2000))\n",
    "        \n",
    "    def calculate_tfidf(self, corpus):\n",
    "        \"\"\" fill in the tfidf matrix with calculated values \"\"\"\n",
    "        self.initiate_matrix(len(corpus))\n",
    "        \n",
    "        idf = self.calculate_idf(corpus)\n",
    "        \n",
    "        for i, doc in enumerate(corpus):\n",
    "            tf = self.calculate_tf(doc)\n",
    "            \n",
    "            for term, value in tf.items():\n",
    "                self.tfidf[i,term] = value * idf[term]            \n",
    "        \n",
    "    def calculate_tf(self, doc) -> defaultdict:\n",
    "        \"\"\" caluclate term frequency for each term in a sentence \"\"\"\n",
    "        tf = dict()\n",
    "        length = len(doc)\n",
    "        \n",
    "        for id, qty in doc.items():\n",
    "            tf[id] = qty / length\n",
    "            \n",
    "        return tf\n",
    "    \n",
    "    def calculate_idf(self, corpus) -> defaultdict:\n",
    "        \"\"\" calculate idf for each term in a corpus \"\"\"\n",
    "        idf = dict()\n",
    "        length = len(corpus)\n",
    "        \n",
    "        for i in range(2000):\n",
    "            counter = 1\n",
    "            for doc in corpus:\n",
    "                if i in doc:\n",
    "                    counter +=1\n",
    "            \n",
    "            idf[i] = log(length / counter)\n",
    "            \n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data.df['comment'].tolist()\n",
    "bpe = myBPEvectorizer(\"data/2k\")\n",
    "bpe.tokenize(docs)\n",
    "ids = bpe.get_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = [Counter(doc) for doc in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.calculate_tfidf(counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss=\"log\", max_iter=30, alpha=0.0001, class_weight='balanced')\n",
    "\n",
    "X = bpe.tfidf\n",
    "y = data.df.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75338189, 0.90426639, 0.90041638, 0.86363636, 0.90319223])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, y, scoring=\"f1_micro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
