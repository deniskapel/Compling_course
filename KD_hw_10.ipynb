{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "KD_hw_10.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bq1kgihgMaId",
        "87Yl6ez_Q2lc",
        "D2Imf1rCRugC",
        "PpCNDMjiXXh2",
        "Qo_4dx2F4Rjz",
        "Dqsl4FVeCUP8",
        "HDOBCv-NCUQA",
        "kxAFu_vDcvNL"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoI6GKLFCUQC"
      },
      "source": [
        "## Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKM7i45qCUQD"
      },
      "source": [
        "**Ваша задача - предложить 3 способа побить бейзлайн на всех данных.**\n",
        "\n",
        "**Baseline**:\n",
        "* Precision -  0.13\n",
        "* Recall -  0.25\n",
        "* F1 -  0.16\n",
        "* Jaccard -  0.09\n",
        "\n",
        "Baseline implemented in here: https://github.com/mannefedov/compling_nlp_hse_course/blob/master/2020/Keyword_extraction.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD3OCLVWCUQD"
      },
      "source": [
        "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 1 балл)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX9Y6SAqDGc4"
      },
      "source": [
        "%%capture\n",
        "!pip3 install pymorphy2[fast]\n",
        "!pip install rake-nltk"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFmYvkQ-CUPs",
        "outputId": "853bb324-fbf9-43b1-80b7-e51ea73c6987"
      },
      "source": [
        "import json, os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from string import punctuation\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from gensim.summarization import keywords\n",
        "from rake_nltk import Rake\n",
        "nltk.download('stopwords')\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))\n",
        "pd.set_option('display.max_colwidth', 100)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq1kgihgMaId"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK4dagLtMg8S"
      },
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSr9FisiMgg4"
      },
      "source": [
        "def evaluate(true_kws, predicted_kws):\n",
        "    assert len(true_kws) == len(predicted_kws)\n",
        "    \n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1s = []\n",
        "    jaccards = []\n",
        "    \n",
        "    for i in range(len(true_kws)):\n",
        "        \n",
        "        true_kw = set(true_kws[i])\n",
        "        predicted_kw = set(predicted_kws[i])\n",
        "        \n",
        "        tp = len(true_kw & predicted_kw)\n",
        "        union = len(true_kw | predicted_kw)\n",
        "        fp = len(predicted_kw - true_kw)\n",
        "        fn = len(true_kw - predicted_kw)\n",
        "        \n",
        "        if (tp+fp) == 0:\n",
        "            prec = 0\n",
        "        else:\n",
        "            prec = tp / (tp + fp)\n",
        "        \n",
        "        if (tp+fn) == 0:\n",
        "            rec = 0\n",
        "        else:\n",
        "            rec = tp / (tp + fn)\n",
        "        if (prec+rec) == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            f1 = (2*(prec*rec))/(prec+rec)\n",
        "            \n",
        "        jac = tp / union\n",
        "        \n",
        "        precisions.append(prec)\n",
        "        recalls.append(rec)\n",
        "        f1s.append(f1)\n",
        "        jaccards.append(jac)\n",
        "    print('Precision - ', round(np.mean(precisions), 2))\n",
        "    print('Recall - ', round(np.mean(recalls), 2))\n",
        "    print('F1 - ', round(np.mean(f1s), 2))\n",
        "    print('Jaccard - ', round(np.mean(jaccards), 2))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WIPst1LMeCu"
      },
      "source": [
        "Normalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H4cXTKqMZk3"
      },
      "source": [
        "class FasterNormalizer():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.morpho = MorphAnalyzer()\n",
        "        self.cashe = {}\n",
        "\n",
        "    def normalize(self, txt: str, tags: list = ['NOUN']) -> list:\n",
        "        \"\"\"\n",
        "            returns lemmas of words with specified POS\n",
        "        \"\"\"\n",
        "        words = self.tokenize(txt)\n",
        "\n",
        "        res = []\n",
        "\n",
        "        for w in words:\n",
        "            if w and w not in stops:\n",
        "                if w in self.cashe:\n",
        "                    if self.cashe[w].tag.POS in tags:\n",
        "                        res.append(self.cashe[w].normal_form)\n",
        "                else:\n",
        "                    r = self.morpho.parse(w)[0]\n",
        "                    if r.tag.POS in tags:\n",
        "                        res.append(r.normal_form)\n",
        "                    self.cashe[w] = r\n",
        "        return res\n",
        "\n",
        "    def normalize_all(self, txt: str) -> list:\n",
        "        \"\"\"\n",
        "            returns only lemmas\n",
        "        \"\"\"\n",
        "        words = self.tokenize(txt)\n",
        "        res = []\n",
        "        for w in words:\n",
        "            if w in self.cashe:\n",
        "                res.append(self.cashe[w].normal_form)\n",
        "            else:\n",
        "                r = self.morpho.parse(w)[0]\n",
        "                res.append(r.normal_form)\n",
        "                self.cashe[w] = r\n",
        "        return res\n",
        "\n",
        "    def tokenize(self, txt:str) -> list:\n",
        "        \"\"\"\n",
        "            tokenizes and removes punctuation from a string\n",
        "        \"\"\"\n",
        "        return [word.strip(punct) for word in txt.lower().split()]\n",
        "\n",
        "morph = FasterNormalizer()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjmUpC5CUPt"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNRz8I7KCUPu"
      },
      "source": [
        "Возьмем данные вот отсюда - https://github.com/mannefedov/ru_kw_eval_datasets Там лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). Датасет НГ самый маленький, поэтому возьмем его в качестве примера."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtgRNy7LCUPu"
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_0.jsonlines.zip\n",
        "!wget https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_1.jsonlines.zip\n",
        "!unzip ng_0.jsonlines.zip \n",
        "!unzip ng_1.jsonlines.zip\n",
        "PATH_TO_DATA = './'\n",
        "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA) if file.endswith('jsonlines')]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PfFyEJKCUPw"
      },
      "source": [
        "Объединим файлы в один датасет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s201VOkbCUPw",
        "outputId": "e62a0d41-d4ce-4aa5-d176-5e98ea8a3a9b"
      },
      "source": [
        "data = pd.concat([pd.read_json(file, lines=True) for file in files][:5], axis=0, ignore_index=True)\n",
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1987, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "RW1FCuGWCUPx",
        "outputId": "df9bb5f3-e5f4-4fde-8418-500a21fa5ef4"
      },
      "source": [
        "data.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keywords</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[школа, образовательные стандарты, литература, история, фгос]</td>\n",
              "      <td>Ольга Васильева обещала \"НГ\" не перегружать школьников</td>\n",
              "      <td>https://amp.ng.ru/?p=http://www.ng.ru/education/2018-03-22/8_7195_school.html</td>\n",
              "      <td>В среду состоялось отложенное заседание Совета по федеральным государственным образовательным ст...</td>\n",
              "      <td>Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов вин...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[красота, законы]</td>\n",
              "      <td>У красоты собственные закон и воля</td>\n",
              "      <td>https://amp.ng.ru/?p=http://www.ng.ru/style/2018-03-19/8_7192_beauty.html</td>\n",
              "      <td>Хорошо, когда красота в глазах смотрящего живет свободно или хотя бы занимает широкий угол зрени...</td>\n",
              "      <td>О живительной пользе укорота при выборе между плохим и хорошим</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[юзефович, гражданская война, пепеляев, якутия]</td>\n",
              "      <td>Апокалиптический бунт</td>\n",
              "      <td>https://amp.ng.ru/?p=http://www.ng.ru/zavisimaya/2017-12-19/15_7139_bunt.html</td>\n",
              "      <td>Когда-то Леонид Юзефович написал книгу о монгольской эпопее барона Унгерна «Самодержец пустыни» ...</td>\n",
              "      <td>Крепость из тел и призрак независимой Якутии</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        keywords  ...                                                                                              summary\n",
              "0  [школа, образовательные стандарты, литература, история, фгос]  ...  Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов вин...\n",
              "1                                              [красота, законы]  ...                                       О живительной пользе укорота при выборе между плохим и хорошим\n",
              "2                [юзефович, гражданская война, пепеляев, якутия]  ...                                                         Крепость из тел и призрак независимой Якутии\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsoIP9a5QXtv",
        "outputId": "7182f906-904f-4a06-ea9a-940de5187872"
      },
      "source": [
        "data.keywords.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [школа, образовательные стандарты, литература, история, фгос]\n",
              "Name: keywords, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9yONhB1CUPy"
      },
      "source": [
        "test metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfq7ciojCUPy",
        "outputId": "e6fafae3-2b76-44ec-e409-89eebf52bd6b"
      },
      "source": [
        "evaluate(data['keywords'], data['keywords'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  1.0\n",
            "Recall -  1.0\n",
            "F1 -  1.0\n",
            "Jaccard -  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGIjLwVRLSD2"
      },
      "source": [
        "Normilize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs04OigVCUP2"
      },
      "source": [
        "data['content_norm'] = data['content'].apply(morph.normalize)\n",
        "data['title_norm'] = data['title'].apply(morph.normalize)\n",
        "data['content_norm_all'] = data['content'].apply(morph.normalize_all)\n",
        "data['title_norm_all'] = data['title'].apply(morph.normalize_all)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WexqrYBrCUP2",
        "outputId": "eaf5dd86-c9a5-4185-e229-0d3285e106a7"
      },
      "source": [
        "data['content_norm'].head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [среда, заседание, совет, стандарт, фгос, министерство, образование, наука, рф, собрание, понеде...\n",
              "1    [красота, глаз, угол, зрение, свет, темень, пустота, зрачок, слава, бог, красота, обход, организ...\n",
              "2    [леонид, юзефович, книга, эпопея, барон, самодержец, пустынь, бестселлер, классика, жанр, роман,...\n",
              "3    [гран-при, испания, евротур, гонка, трасса, барселона, календарь, минимум, год, автодром, старое...\n",
              "4    [год, версия, убийство, есенин, чекист, сотрудник, гпу, телесериал, есенин, виновник, смерть, по...\n",
              "5    [главное, государство, цель, образование, год, обучение, практика, выпускник, диплом, аккредитац...\n",
              "6    [носитель, космос, вселенная, человек, язык, полёт, универсум, долгопрудный, дом, интеллект, тир...\n",
              "7    [полгода, заявление, джордж, буш, завершение, война, ирак, война, американец, сентябрь, принцип,...\n",
              "8    [время, философия, категория, понятие, сущность-элемент, отношение, действительность, знание, ис...\n",
              "9    [требование, страна, россия, переговоры, вступление, организация, вто, выравнивание, тариф, газ,...\n",
              "Name: content_norm, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWZejnUiLgbm",
        "outputId": "2110e8a9-f69e-4286-ba16-c44bdad412a9"
      },
      "source": [
        "data['content_norm_all'].head(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [в, среда, состояться, отложить, заседание, совет, по, федеральный, государственный, образовател...\n",
              "1    [хорошо, когда, красота, в, глаз, смотреть, жить, свободно, или, хотя, бы, занимать, широкий, уг...\n",
              "2    [когда-то, леонид, юзефович, написать, книга, о, монгольский, эпопея, барон, унгерный, самодерже...\n",
              "3    [гран-при, испания, открыть, евротур, формулы-1, гонка, на, трасса, близ, барселона, пока, остав...\n",
              "4    [десять, год, назад, быть, популярный, версия, убийство, есенин, чекист, точнее, сотрудник, гпу,...\n",
              "5    [, начать, с, главное, считать, что, поставить, государство, цель, абсолютно, правильный, мы, по...\n",
              "6    [в, нынешний, бумажный, носитель, сойтись, как, всегда, почти, непреднамеренный, три, космос, вс...\n",
              "7    [едва, ли, минуть, полгода, после, торжественный, заявление, джордж, буш, о, победоносный, завер...\n",
              "8    [с, античный, время, философия, определять, категория, как, наиболее, общий, понятие, сущность-э...\n",
              "9    [как, известно, один, из, требование, страна, с, который, россия, вести, переговоры, по, вступле...\n",
              "Name: content_norm_all, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Yl6ez_Q2lc"
      },
      "source": [
        "## Approach 1: TFIDF limited to unigrams\n",
        "\n",
        "Title+content\n",
        "* Precision -  0.14 (+0.01)\n",
        "* Recall -  0.28 (+0.03)\n",
        "* F1 -  0.18 (+0.02)\n",
        "* Jaccard -  0.11 (+0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Imf1rCRugC"
      },
      "source": [
        "### A: Keys words are mostly nouns. Bigrams and trigrams add a lot of garbage, so if we just limit it to unigrams and take top 10 tokens, the results are higher (all four metrics)\n",
        "\n",
        "* Precision -  0.14 (+0.01)\n",
        "* Recall -  0.27 (+0.02)\n",
        "* F1 -  0.18 (+0.02)\n",
        "* Jaccard -  0.1 (+0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP7Ryzf_CUP5"
      },
      "source": [
        "Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLB6QhwECUP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5434a3fd-07f8-4233-82a1-aae87d77cd52"
      },
      "source": [
        "data['content_norm_str'] = data['content_norm'].apply(' '.join)\n",
        "# use bigrams two\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,1), min_df=2)\n",
        "#vectorize text\n",
        "tfidf.fit(data['content_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
        "# transform texts intp vectors with coefficients from id2word\n",
        "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
        "\n",
        "\n",
        "keywords = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    # it's a sparce matrix, so extract content\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    # transform into array and take top 10 tokens\n",
        "    top_ids = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    # id 2 word\n",
        "    keywords.append([id2word[w] for w in top_ids])\n",
        "\n",
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.14\n",
            "Recall -  0.27\n",
            "F1 -  0.18\n",
            "Jaccard -  0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpCNDMjiXXh2"
      },
      "source": [
        "### B (same approach): Playing with a number of extracting words\n",
        "\n",
        "Same approach, but only top 5 tokens are taken. In general, decresing the number of tokens significantly improves precision (+0.15 if decreased to 1), but recall drops down. Other metrics rise a little, too. \n",
        "\n",
        "* Precision - 0.2 (+0.07)\n",
        "* Recall - 0.2 (-0.05)\n",
        "* F1 - 0.19 (+0.03)\n",
        "* Jaccard - 0.12 (+0.03)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeGQuQGLYDZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab8611f-e723-4e40-c35d-958342aa1e0c"
      },
      "source": [
        "keywords = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    # it's a sparce matrix, so extract content\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    # transform into array and take top 10 tokens\n",
        "    top_ids = row_data.toarray().argsort()[0,:-6:-1]\n",
        "    # id 2 word\n",
        "    keywords.append([id2word[w] for w in top_ids])\n",
        "\n",
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.2\n",
            "Recall -  0.2\n",
            "F1 -  0.19\n",
            "Jaccard -  0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo_4dx2F4Rjz"
      },
      "source": [
        "### C: join title and content\n",
        "\n",
        "Slight improvement:\n",
        "* Precision -  0.14\n",
        "* Recall -  0.28 (+0.03 to A)\n",
        "* F1 -  0.18\n",
        "* Jaccard -  0.11 (+0.01 to A)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZgrqA5D4W86",
        "outputId": "587b2198-92b0-46b6-966a-72be36efdb8a"
      },
      "source": [
        "data['content_norm_str'] = data['title_norm'].apply(' '.join) + data['content_norm'].apply(' '.join)\n",
        "# use bigrams two\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,1), min_df=2)\n",
        "#vectorize text\n",
        "tfidf.fit(data['content_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
        "# transform texts intp vectors with coefficients from id2word\n",
        "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
        "\n",
        "keywords = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    # it's a sparce matrix, so extract content\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    # transform into array and take top 10 tokens\n",
        "    top_ids = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    # id 2 word\n",
        "    keywords.append([id2word[w] for w in top_ids])\n",
        "\n",
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.14\n",
            "Recall -  0.28\n",
            "F1 -  0.18\n",
            "Jaccard -  0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqsl4FVeCUP8"
      },
      "source": [
        "## Approach 2: Graphs\n",
        "\n",
        "Testing different centrality measures from https://networkx.org/documentation/stable/reference/algorithms/centrality.html#current-flow-betweenness\n",
        "\n",
        "Various centrality measures **do not** seem to improve the performance.\n",
        "\n",
        "Best score (PageRank on nouns only) is the same as the baseline:\n",
        "* Precision -  0.13\n",
        "* Recall -  0.25\n",
        "* F1 -  0.16\n",
        "* Jaccard -  0.09\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6VX0CRlCUP_"
      },
      "source": [
        "def build_matrix(text, window_size=5):\n",
        "    vocab = set(text)\n",
        "    word2id = {w:i for i, w in enumerate(vocab)}\n",
        "    id2word = {i:w for i, w in enumerate(vocab)}\n",
        "    # преобразуем слова в индексы для удобства\n",
        "    ids = [word2id[word] for word in text]\n",
        "\n",
        "    # создадим матрицу совстречаемости\n",
        "    m = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "    # пройдемся окном по всему тексту\n",
        "    for i in range(0, len(ids), window_size):\n",
        "        window = ids[i:i+window_size]\n",
        "        # добавим единичку всем парам слов в этом окне\n",
        "        for j, k in combinations(window, 2):\n",
        "            # чтобы граф был ненаправленный \n",
        "            m[j][k] += 1\n",
        "            m[k][j] += 1\n",
        "    \n",
        "    return m, id2word\n",
        "\n",
        "def some_centrality_measure(text, centrality_metrics = nx.pagerank, window_size=5, topn=5):\n",
        "    \n",
        "    matrix, id2word = build_matrix(text, window_size)\n",
        "    G = nx.from_numpy_array(matrix)\n",
        "    # тут можно поставить любую метрику\n",
        "    # менять тут \n",
        "    node2measure = dict(centrality_metrics(G)) \n",
        "    \n",
        "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSB6i2iVCUP_"
      },
      "source": [
        "PageRank by default, other metrics come from here - https://networkx.github.io/documentation/stable/reference/algorithms/centrality.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_l7SrboCUP_",
        "outputId": "83d34c75-6b9a-443b-adc5-fc52f13d5273"
      },
      "source": [
        "keyword_nx = data['content_norm'].apply(\n",
        "    lambda x: some_centrality_measure(x, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.13\n",
            "Recall -  0.25\n",
            "F1 -  0.16\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdAefwk6CUP_",
        "outputId": "a15b1fbe-b103-4208-a3e6-4d82971f37de"
      },
      "source": [
        "keyword_nx = data['content_norm_all'].apply(\n",
        "    lambda x: some_centrality_measure(x, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.06\n",
            "Recall -  0.12\n",
            "F1 -  0.07\n",
            "Jaccard -  0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-LMClJxCUQA",
        "outputId": "80ad129d-8059-4a75-fea8-72293573446e"
      },
      "source": [
        "keyword_nx = data['content_norm'].apply(\n",
        "    lambda x: some_centrality_measure(\n",
        "        x, centrality_metrics = nx.load_centrality, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.12\n",
            "Recall -  0.23\n",
            "F1 -  0.15\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTA-2yxAOQAj",
        "outputId": "5fdc81de-37e6-4d22-ec7f-032dd44ec351"
      },
      "source": [
        "keyword_nx = data['content_norm_all'].apply(\n",
        "    lambda x: some_centrality_measure(\n",
        "        x, centrality_metrics = nx.load_centrality, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.05\n",
            "Recall -  0.11\n",
            "F1 -  0.07\n",
            "Jaccard -  0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8uRc9GP3OqU",
        "outputId": "38b2bc46-c06e-43d7-d483-b80de8aa5852"
      },
      "source": [
        "keyword_nx = data['content_norm'].apply(\n",
        "    lambda x: some_centrality_measure(\n",
        "        x, centrality_metrics = nx.subgraph_centrality, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.13\n",
            "Recall -  0.25\n",
            "F1 -  0.16\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCk6wq4TObNS",
        "outputId": "16e198df-2e45-47ea-bf81-4297b7c8ac16"
      },
      "source": [
        "keyword_nx = data['content_norm_all'].apply(\n",
        "    lambda x: some_centrality_measure(\n",
        "        x, centrality_metrics = nx.subgraph_centrality, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.06\n",
            "Recall -  0.12\n",
            "F1 -  0.08\n",
            "Jaccard -  0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjO18eC0PNlc"
      },
      "source": [
        "if only on titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgZxjkj4PQ45",
        "outputId": "4bdd336a-2588-4040-926d-637d4845009b"
      },
      "source": [
        "keyword_nx = data['title_norm'].apply(\n",
        "    lambda x: some_centrality_measure(x, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.2\n",
            "Recall -  0.13\n",
            "F1 -  0.14\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47bd-rHPZUz",
        "outputId": "51917f4c-5892-4d3c-a814-2c61c9929b21"
      },
      "source": [
        "keyword_nx = data['title_norm_all'].apply(\n",
        "    lambda x: some_centrality_measure(x, window_size=10, topn=10))\n",
        "\n",
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.11\n",
            "Recall -  0.13\n",
            "F1 -  0.11\n",
            "Jaccard -  0.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDOBCv-NCUQA"
      },
      "source": [
        "## Approach 3: Gensim\n",
        "\n",
        "Gensim works significantly worse\n",
        "\n",
        "Best score:\n",
        "\n",
        "* Precision -  0.07 (-0.06)\n",
        "* Recall -  0.11 (-0.14)\n",
        "* F1 -  0.08 (-0.08)\n",
        "* Jaccard -  0.04 (-0.05)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsaqwgcKCUQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cda6fb-f155-464a-99fc-4741f9040c68"
      },
      "source": [
        "gensim_kws = data['content_norm'].apply(lambda x: keywords(' '.join(x)).split('\\n')[:10])\n",
        "evaluate(data['keywords'], gensim_kws)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.07\n",
            "Recall -  0.11\n",
            "F1 -  0.08\n",
            "Jaccard -  0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD2fR_T5CUQC",
        "outputId": "1f51feb0-8b62-4cd1-ecc8-fb658dd41fcc"
      },
      "source": [
        "gensim_kws = data['content_norm_all'].apply(lambda x: keywords(' '.join(x)).split('\\n')[:10])\n",
        "evaluate(data['keywords'], gensim_kws)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.05\n",
            "Recall -  0.09\n",
            "F1 -  0.06\n",
            "Jaccard -  0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiuMMXYMRRbH",
        "outputId": "7c0af9fa-e694-4f56-bf46-3da42364cb68"
      },
      "source": [
        "gensim_kws = data['title_norm'].apply(lambda x: keywords(' '.join(x)).split('\\n')[:10])\n",
        "evaluate(data['keywords'], gensim_kws)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.03\n",
            "Recall -  0.01\n",
            "F1 -  0.01\n",
            "Jaccard -  0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcutMzPwRUun",
        "outputId": "0852b2a8-cbb4-413b-e706-8809a96003ea"
      },
      "source": [
        "gensim_kws = data['title_norm_all'].apply(lambda x: keywords(' '.join(x)).split('\\n')[:10])\n",
        "evaluate(data['keywords'], gensim_kws)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.03\n",
            "Recall -  0.01\n",
            "F1 -  0.01\n",
            "Jaccard -  0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxAFu_vDcvNL"
      },
      "source": [
        "## Approach 4: RAKE\n",
        "\n",
        "https://pypi.org/project/rake-nltk/\n",
        "\n",
        "A bit better than the baseline:\n",
        "* Precision -  0.14 (+0.01)\n",
        "* Recall -  0.27 (+0.02)\n",
        "* F1 -  0.17 + (0.01)\n",
        "* Jaccard -  0.1 (0.01)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6QB2movdKyX"
      },
      "source": [
        "r = Rake(language='russian',\n",
        "    stopwords=stops,\n",
        "    punctuations=punct\n",
        ")"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Sidb-cjZ5_",
        "outputId": "ff153a5e-ade4-4b3e-e0cc-0db0d6443764"
      },
      "source": [
        "keywords = []\n",
        "\n",
        "for title, content in zip(data['title_norm'].tolist(), data['content_norm'].tolist()):\n",
        "    r.extract_keywords_from_text(\" \".join(title + content))\n",
        "    tmp = [word for ph in r.get_ranked_phrases() for word in ph.split(' ')]\n",
        "    c = Counter(tmp)\n",
        "    keywords.append([keys for keys, items in c.most_common(10)])\n",
        "\n",
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.14\n",
            "Recall -  0.27\n",
            "F1 -  0.17\n",
            "Jaccard -  0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oO9wsb5gUlw"
      },
      "source": [
        "## Overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rTF1UV5gQH8"
      },
      "source": [
        "In general, all these approaches refer to unsupervised. In order to test supervised, e.g. LDA, another larger dataset needs to be prepared to train and test on. The best unsupervised approach is TFIDF."
      ]
    }
  ]
}