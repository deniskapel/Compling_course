{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "KD_hw_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WKxsdx2WD3D"
      },
      "source": [
        "### Домашнее задание. Реализовать алгоритм Леска и проверить его на реальном датасете"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJKqsymfWD3D"
      },
      "source": [
        "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWOme69N5p9N"
      },
      "source": [
        "**Дополнительный балл**\n",
        "Если хотите заработать дополнительный балл, попробуйте улучшить алгоритм Леска любым способом (например, использовать расстояние редактирования вместо пересечения или даже вставить машинное обучение)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW6jP5at4qQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486147e8-81a3-4b44-cbb3-2ec5c1ac72b0"
      },
      "source": [
        "!pip install fuzzywuzzy[speedup]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy[speedup]\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Collecting python-levenshtein>=0.12; extra == \"speedup\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup]) (56.1.0)\n",
            "Building wheels for collected packages: python-levenshtein\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149798 sha256=140adb349b15d0a84d56d9c79ebe55d710529a838d5db9151ea643f5fb5fd971\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "Successfully built python-levenshtein\n",
            "Installing collected packages: python-levenshtein, fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0 python-levenshtein-0.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETu-RbELxeta",
        "outputId": "5bb73c42-b9c0-499a-8e05-226be29579e2"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from fuzzywuzzy import fuzz\n",
        "from string import punctuation\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUlQM2ehWD3D"
      },
      "source": [
        "def fuzzy_compare(sent1: str, sent2: str) -> int:\n",
        "    \"\"\" one of the options to compare an example and a definition in Lesk algo\"\"\"\n",
        "    return fuzz.token_sort_ratio(sent1, sent2)\n",
        "\n",
        "def lesk(word: str, sentence: str, compare_how: callable) -> int:\n",
        "    \"\"\" \n",
        "        disambiguate a given word using nltk.wordnet\n",
        "        return an index of a word the word's lemma is not known.\n",
        "        Pass a part of speech first\n",
        "    \"\"\"\n",
        "    bestsense = 0\n",
        "    maxoverlap = 0\n",
        "    word=wn.morphy(word) if wn.morphy(word) is not None else word\n",
        "    synsets = wn.synsets(word)\n",
        " \n",
        "    for i, syns in enumerate(synsets):\n",
        "        overlap = compare_how(sentence, syns.definition())\n",
        "                \n",
        "        if overlap > maxoverlap:\n",
        "            maxoverlap = overlap\n",
        "            bestsense = i\n",
        "    return bestsense"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf4NuGTWD3E",
        "outputId": "5981f9cd-c880-4dfe-8955-1a96fca1136a"
      },
      "source": [
        "# на вход подается элемент результата работы уже написанной вами функции get_words_in_context\n",
        "lesk('day', 'some point or period in time', compare_how=fuzzy_compare) # для примера контекст совпадает с одним из определений\n",
        "# а на выходе индекс подходящего синсета"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJn76IJRWD3F"
      },
      "source": [
        "**Проверьте насколько хорошо работает такой метод на реальном датасете.** http://lcl.uniroma1.it/wsdeval/ - большой фреймворк для оценки WSD. Там много данных и я взял кусочек, чтобы не было проблем с памятью"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohRIvLiOaVA2"
      },
      "source": [
        "Вам нужно для каждого многозначного слова (т.е. у него есть тэг в первом поле) с помощью алгоритма Леска предсказать нужный синсет и сравнить с правильным. Посчитайте процент правильных предсказаний (accuracy).\n",
        "\n",
        "Если считается слишком долго, возьмите поменьше предложений (например, только тысячу)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPy2WxYmWD3F"
      },
      "source": [
        "corpus_wsd = []\n",
        "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
        "for sent in corpus:\n",
        "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps8UJjci_EnA",
        "outputId": "4f17794d-03e3-4a69-fe1e-3430cba6c42a"
      },
      "source": [
        "true = 0\n",
        "total = 0\n",
        "\n",
        "for sent in corpus_wsd[0:1000]:\n",
        "    for word in sent:\n",
        "        meaning, lemma, word = word\n",
        "        if not meaning:\n",
        "            continue\n",
        "        total += 1\n",
        "        context = \" \".join([s[2] for s in sent])\n",
        "        meaning_id = lesk(word, context, compare_how=fuzzy_compare)\n",
        "        try:\n",
        "            if wn.synsets(lemma)[meaning_id] == wn.lemma_from_key(meaning).synset():\n",
        "                true += 1\n",
        "        except:\n",
        "            # somehow, several meaning_ids are out of range\n",
        "            pass\n",
        "\n",
        "print('percentage of correct: %f' % (round((true/total) * 100, 1)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "percentage of correct: 37.600000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}