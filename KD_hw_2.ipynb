{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwUaGt5avGb_"
   },
   "source": [
    "# Task 1\n",
    "В тетрадке реализована биграмная языковая модель (при генерации учитывается информация только о 1 предыдущем слове). Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами, сгенерированными биграмной моделью. \n",
    "Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке).  \n",
    "\n",
    "Делать это задание будет легче после прочтения первых 7 страниц вот этой главы из Журафского - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "Y1CnpfN3vGb_"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "class Preporcessor():\n",
    "    \"\"\"\n",
    "        a class to preprocess a txt file availabe through the path\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path # a path to a txt file\n",
    "        self.text = \"\"\n",
    "        self.tokens_by_sentence = []\n",
    "        \n",
    "    def extractContent(self):\n",
    "        \"\"\" loads the content of the file \"\"\"\n",
    "        with open(self.path, 'r', encoding=\"utf-8\") as f: # open in readonly mode\n",
    "            self.text = f.read()\n",
    "            \n",
    "    def tokenize_by_sentence(self, n=0):\n",
    "        \"\"\"\n",
    "            tokenizes a text into a list of tokens by sentence\n",
    "            inserts n times <start> and n times <end> of tags\n",
    "            at the beginning and the end of a sentence\n",
    "        \"\"\"\n",
    "        tmp = [self.sp_tags(self.normalize_text(sent), n) for sent in self.get_sentences()]\n",
    "        self.tokens_by_sentence = tmp\n",
    "    \n",
    "    def get_sentences(self) -> list:\n",
    "        \"\"\" uses nltk sent_tokenize to split the text into a list of sentences \"\"\"\n",
    "        return(sent_tokenize(self.text[0:250000])) # limit the coprus\n",
    "\n",
    "    def normalize_text(self, text) -> list:\n",
    "        \"\"\" normalizes text and turns it into a list of tokens \"\"\"\n",
    "        to_remove = punctuation + '«»“”'\n",
    "        tmp = [word.text.strip(to_remove) for word in razdel_tokenize(text)]\n",
    "        tmp = [word.lower() for word in tmp if word and len(word) < 20]\n",
    "        return(tmp)\n",
    "    \n",
    "    def sp_tags(self, tokens, n=0) -> list:\n",
    "        \"\"\" recieves a list of tokens and returns them with special tags \"\"\"\n",
    "        if n >= 0:\n",
    "            return(['<start>'] * n + tokens + ['<end>'] *n)\n",
    "        else:\n",
    "            return(ValueError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg3Jk6BqvGcA"
   },
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "Jkh11LtlvGcA"
   },
   "outputs": [],
   "source": [
    "news = Preprocessor('data/lenta.txt')\n",
    "news.extractContent()\n",
    "news.tokenize_by_sentence(2) # add 2 special tokens\n",
    "# news.tokenize_by_sentence(0) # add no special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqKfK5YsvGcA"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "id": "awHKTcyjvGcA"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class TextGenerator():\n",
    "    \"\"\"\n",
    "        a class to generate text based on a list tokens split by sentences\n",
    "        input: [['tokens', 'of', 'a', 'single', 'sentence']]\n",
    "        \n",
    "        !!! modify to work on any ngram >= 2. Now it does not suit any to the full extent !!!\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens, ngram_length=2):\n",
    "        if ngram_length < 2: # for bigrams and larger series only\n",
    "            return(ValueError)\n",
    "        self.tokens = tokens\n",
    "        self.ngram_length = ngram_length\n",
    "        self.ngrams = [] # [0] (n-1)grams [1] ngrams\n",
    "        self.id2unigram = []\n",
    "        self.unigram2id = {}\n",
    "        self.id2ngram = []\n",
    "        self.ngram2id = {}\n",
    "        self.matrix = np.empty(0)\n",
    "        \n",
    "     \n",
    "    def generate_ngrams(self):\n",
    "        \"\"\" generate ngrams from sentences \"\"\"\n",
    "        self.generate_counters() # generate enough counters for ngrams\n",
    "        \n",
    "        for sentence in self.tokens:\n",
    "            self.ngrams[0].update(sentence)\n",
    "            self.ngrams[1].update(\n",
    "                self.ngrammer(sentence, (self.ngram_length-1)) # gerenate ngrams of n-1 lenght\n",
    "                ) \n",
    "            self.ngrams[2].update(\n",
    "                self.ngrammer(sentence, self.ngram_length) # generate ngrams\n",
    "                )\n",
    "\n",
    "    def generate_counters(self):\n",
    "        \"\"\"\n",
    "            generates a number of ngrams Counter() to store in self.ngrams\n",
    "        \"\"\"\n",
    "        [self.ngrams.append(Counter()) for i in range(3)]\n",
    "        \n",
    "    def ngrammer(self, tokens, n=2) -> list:\n",
    "        \"\"\" return ngrams \"\"\"\n",
    "        ngrams = []\n",
    "        for i in range(0,len(tokens)-n+1):\n",
    "            ngrams.append(' '.join(tokens[i:i+n]))\n",
    "        return(ngrams)\n",
    "    \n",
    "    def build_matrix(self):\n",
    "        self.matrix = np.zeros(\n",
    "            (len(self.ngrams[1]), # rows with bigrams\n",
    "             len(self.ngrams[0]) # columns with unigrams\n",
    "            ))\n",
    "        \n",
    "        self.id2unigram = list(self.ngrams[0]) # index unigrams\n",
    "        self.unigram2id = {unigram:i for i, unigram in enumerate(self.id2unigram)}\n",
    "        self.id2ngram = list(self.ngrams[1]) # index (n-1)grams\n",
    "        self.ngram2id = {ngram:i for i, ngram in enumerate(self.id2ngram)}\n",
    "        \n",
    "        \n",
    "        for ngram in self.ngrams[-1]:\n",
    "            tmp = ngram.split()\n",
    "            prev = \" \".join(tmp[:-1]) # save ngram without last token\n",
    "            self.matrix[self.ngram2id[prev]][self.unigram2id[tmp[-1]]] = (self.ngrams[-1][ngram]/\n",
    "                                                                            self.ngrams[-2][prev])\n",
    "            \n",
    "    def generate_text(self, n=100, start='<start> <start>') -> str:\n",
    "        \"\"\" generates text with a calculated matrix \"\"\"\n",
    "        text = []\n",
    "        current_idx = self.ngram2id[start]\n",
    "    \n",
    "        for i in range(n):\n",
    "\n",
    "            chosen = np.random.choice(self.matrix.shape[1], p=self.matrix[current_idx])\n",
    "            text.append(self.id2unigram[chosen])\n",
    "\n",
    "            if self.id2unigram[chosen] != '<end>':\n",
    "                current_idx = self.ngram2id[\n",
    "                    self.id2ngram[current_idx].split()[-1] + \" \" + self.id2unigram[chosen]]\n",
    "            else:\n",
    "                current_idx = self.ngram2id[start]\n",
    "                \n",
    "        return '  '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "SJCjLjkXvGcA"
   },
   "outputs": [],
   "source": [
    "news_generator = TextGenerator(news.tokens_by_sentence, 3) # to be modified to work on any ngram >= 2\n",
    "news_generator.generate_ngrams()\n",
    "news_generator.build_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuFqgchLx0xM",
    "outputId": "c4e336ad-0844-4d55-cd7c-0a6d78bac968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "об  этом  сегодня  пишут  все  газеты  мира  сказал  лидер  яблока  \n",
      "  сша  планирующие  развернуть  новую  национальную  систему  противоракетной  обороны  хотят  добиться  изменений  в  договоре  по  про  поскольку  настоящая  версия  договора  не  позволяет  развертывание  нпро  \n",
      "  ожидается  что  в  пятницу  генпрокуратура  рф  отстранило  трех  ведущих  следователей  от  расследования  других  дел  о  коррупции  вооруженные  отряды  из  восточнотиморской  столицы  дили  официальный  представитель  генерального  секретаря  оон  фред  экхардзаявил  в  минувший  вторник  в  американской  печати  появились  выдержки  из  служебного  письма  адресованного  председателем  совета  директоров  мвф  доклад  о  результатах  своих  переговоров  с  возглавляемым  сша  командованием  сил  оон  провозгласила  свою  собственую  морскую  границу  60-70  километрами  южнее  существующей\n"
     ]
    }
   ],
   "source": [
    "print(news_generator.generate_text().replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3wauLCO03bo",
    "outputId": "ddba85b4-f6ab-4b8a-9214-dfaea60bd523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "как  заявили  интерфаксу  в  четверг  подал  в  останкинский  межмуниципальный  суд  иск  к  орт  о  защите  чести  и  достоинства  \n",
      "  в  рамках  мандата  позволяющего  миротворцам  применять  силу  для  самозащиты  \n",
      "  входе  перестрелки  с  правительственными  войсками  один  боевиксорвался  в  пропасть  и  предположительно  погиб  \n",
      "  в  вышедшей  в  эфир  как  сообщила  пресс-службе  штаба  боос  выступил  с  протестом  против  вольной  трактовки  своего  интервью  расценив  ее  как  намеренное  искажение  его  слов  и  мыслей  \n",
      "  главным  образом  считает  источник  это  связано  с  тем  возможность  переговоров  и  исламистами  вовсе  не  означает  сказал  махтигаджиев  что  все  миротворцы  следуют  этому  призыву  констатирует  мид  рф  распространил  сообщение  в  котором  говорится\n"
     ]
    }
   ],
   "source": [
    "print(news_generator.generate_text().replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9SvFwOU048_",
    "outputId": "d06d36e3-8bb1-4e55-c91a-c84a38d10b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в  итоге  фас  мо  удовлетворил  кассационные  жалобы  инкомбанка  и  его  секретные  коды  \n",
      "  об  этом  интерфаксу  сообщили  что  дневной  заработок  хлебного  спекулянта  достигает  минимум  ста  рублей  но  пресечь  этот  бизнес  нельзя  поскольку  перепродажа  если  она  не  связана  с  проведением  осенней  сессии  руководящих  органов  мвф  и  российским  правительством  и  за  его  голову  фбр  обещаловыплатить  5  млн  рублей  \n",
      "  содержание  алкоголя  в  крови  водителя  лимузина  генри  пола  в  три  раза  превосходило  норму  1,8  грамма  на  литр  \n",
      "  как  сообщили  интерфаксу  в  четверг  вечером  представители  генеральной  прокуратуры  россии  приступили  к  обыскам  на  даче  и  в  еще  одной  большой  стране  по  выражению  александра  лившица\n"
     ]
    }
   ],
   "source": [
    "print(news_generator.generate_text().replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvsH7_iC9BcB"
   },
   "source": [
    "# Task 2\n",
    "Задание 2. (5 баллов) Напишите функцию оценивания нграммов на основе PMI. Используйте эту функцию вместо дефолтной в gensim.models.Phrases Обучите два последовательных модели Phrases с такой функцией и проанализируйте результаты, получаемые после преобразования текстов двумя Phrases.\n",
    "\n",
    "Пояснения: Формулу PMI можно посмотреть вот тут https://en.wikipedia.org/wiki/Pointwise_mutual_information , также там описано как вывести вероятности из количества вхождений слова1, слова2, нграмма и размера корпуса. Чтобы функцию можно было поставить в аргумент scoring в Phrases у нее должны быть вот такие аргументы - scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count) Подберите параметр threshold под эту функцию. Чтобы проверить, что она вообще работает поставьте какое-то совсем маленькое число, чтобы порог проходили все нграммы и потом постепенно его повышайте. В тетрадке есть пример обучения нескольких Phrases последовательно, воспользуйтесь им."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9bxfkhLDvGcB",
    "outputId": "95135963-da76-4eeb-e83b-b161c91cb265"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "def scorer_pmi(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count):\n",
    "    \"\"\" \n",
    "        a function to evaluate ngrams to be used in gensim.Phraser\n",
    "        pmi formula is based on table retrieved from the following link\n",
    "        https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = np.log(\n",
    "            (bigram_count / worda_count) / \n",
    "            (wordb_count / corpus_word_count))\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uNMEsPHbvGcB",
    "outputId": "db6f025e-302a-475d-d631-91947c8bd944"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.034908170336502"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer_pmi(1938, 1311, 1159, 1000, 1000, 50000952)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дефолтная функция на низком пороге работает более устойчиво, не захватывая лишний мусор, созданная функция захватывает почти любые последовательсности\n",
    "\n",
    "На более высоком пороге, дефолтная функция также лучше справляется, позволяя устанавливать цепочки длиннее: 'со_ссылкой_на', 'в_том_числе'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_AtZMoIevGcB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['однако', 'уточненная', 'оценка', 'числа', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'достигнуть', 'ста', 'человек']\n",
      "['агентство', 'итар-тасс', 'в', 'сообщении', 'от', '21.15', 'со_ссылкой_на', 'источники', 'в', 'гувд', 'москвы', 'говорит', 'только', 'о', '30', 'раненых', 'в_том_числе', 'о', 'двух', 'пострадавших', 'в', 'тяжелом', 'состоянии']\n",
      "['однако', 'число', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'составить', 'до', 'ста', 'человек']\n",
      "['по_данным', 'риа_новости', 'боткинская', 'больница', 'институт', 'им']\n",
      "['склифосовского', '1-ая', 'градская', '36-ая', 'и', '64-ая', 'горбольница', 'работают', 'только', 'на', 'прием', 'пострадавших']\n"
     ]
    }
   ],
   "source": [
    "corpus = news.tokens_by_sentence[0:50000]\n",
    "# собираем статистики\n",
    "ph = gensim.models.Phrases(corpus, threshold = 15, scoring='default')\n",
    "# преобразовывать можно и через ph, но так быстрее \n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "# собираем статистики по уже забиграммленному тексту\n",
    "ph2 = gensim.models.Phrases(p[corpus])\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "for i in range(5):\n",
    "    print(p2[p[corpus[50+i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['однако', 'уточненная', 'оценка', 'числа', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'достигнуть', 'ста', 'человек']\n",
      "['агентство', 'итар-тасс', 'в', 'сообщении', 'от', '21.15', 'со_ссылкой', 'на', 'источники', 'в', 'гувд', 'москвы', 'говорит', 'только', 'о', '30', 'раненых', 'в_том', 'числе', 'о', 'двух', 'пострадавших', 'в', 'тяжелом', 'состоянии']\n",
      "['однако', 'число', 'пострадавших', 'в_результате', 'этого', 'взрыва', 'может', 'составить', 'до', 'ста', 'человек']\n",
      "['по_данным', 'риа_новости', 'боткинская', 'больница', 'институт', 'им']\n",
      "['склифосовского', '1-ая', 'градская', '36-ая', 'и', '64-ая', 'горбольница', 'работают', 'только', 'на', 'прием', 'пострадавших']\n"
     ]
    }
   ],
   "source": [
    "corpus = news.tokens_by_sentence[0:50000]\n",
    "# собираем статистики\n",
    "ph = gensim.models.Phrases(corpus, threshold = 15, scoring=scorer_pmi)\n",
    "# преобразовывать можно и через ph, но так быстрее \n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "# собираем статистики по уже забиграммленному тексту\n",
    "ph2 = gensim.models.Phrases(p[corpus])\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "for i in range(5):\n",
    "    print(p2[p[corpus[50+i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gensim.models.phrases.original_scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2.scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black white'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = 'black white blue'.split()\n",
    "\" \".join(series[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <start> <start> <start>'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(['<start>']*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KD_hw_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
