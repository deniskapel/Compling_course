{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import operator\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "from collections import Counter\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    \"\"\"\n",
    "        a class to preprocess a txt file availabe through the path\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path # a path to a txt file\n",
    "        self.content = \"\"\n",
    "        self.clean = \"\"\n",
    "        \n",
    "    def getFileContent(self):\n",
    "        \"\"\"\n",
    "            loads the content of the file\n",
    "        \"\"\"\n",
    "        with open(self.path, 'r', encoding=\"utf8\") as f: # open in readonly mode\n",
    "            # do your stuff\n",
    "            self.content = f.read()\n",
    "            \n",
    "    def removeJunk(self):\n",
    "        \"\"\"\n",
    "            removes tags and other garbage with re module\n",
    "        \"\"\"\n",
    "        pattern = re.compile(\n",
    "                (r'(<[^>]+>)' # tags\n",
    "                 r'|'\n",
    "                 r'([A-Za-z0-9\\+/]\\.?\\-?)' # latin chars, digits and irrelevant punkt \n",
    "                 ), re.VERBOSE)\n",
    "        tmp = re.sub(pattern, ' ', self.content)\n",
    "        tmp = re.sub('(  +)', ' ', tmp) # remove space sequences\n",
    "        tmp = re.sub('(\\s\\s+)', '. ', tmp) # remove blank lines.\n",
    "    \n",
    "        self.clean = tmp\n",
    "        \n",
    "    def getSentences(self, text) -> list:\n",
    "        \"\"\"\n",
    "            uses DeepPavlov's rusentokenize to split the text into sentences\n",
    "            by default it uses a self.clean attribute\n",
    "            received after self.removeJunk()\n",
    "        \"\"\"\n",
    "        return(ru_sent_tokenize(text))\n",
    "    \n",
    "    \n",
    "    def tokenize(self, txt) -> list:\n",
    "        \"\"\"\n",
    "            removes any punctuation from a string\n",
    "            return a list of tokenized lowercase string\n",
    "        \"\"\"\n",
    "        punkt = string.punctuation + '«»—…–“”'\n",
    "        tokens = []\n",
    "        \n",
    "        for word in list(razdel_tokenize(txt)):\n",
    "            token = word.text.strip(punkt).lower() # remove punctuation\n",
    "            if token == \"\": # skip empty elements\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "\n",
    "        return(tokens)\n",
    "    \n",
    "    def filter_tokens_by_length(self, tokens, relation, length):\n",
    "        \"\"\"\n",
    "            uses str(operators) and length of the string to filter list of strings \n",
    "        \"\"\"\n",
    "        ops = {'>': operator.gt,\n",
    "               '<': operator.lt,\n",
    "               '>=': operator.ge,\n",
    "               '<=': operator.le,\n",
    "               '=': operator.eq}\n",
    "        \n",
    "        return([token for token in tokens if ops[relation](len(token), length)])\n",
    "    \n",
    "    \n",
    "    def group_sentences(self, tokens_by_sentences) -> list:\n",
    "        \"\"\"\n",
    "            receives lists of tokens grouped by sentences\n",
    "            turns them into lowercase string with no punctuation\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for sentence in tokens_by_sentences:\n",
    "            sentences.append(\" \".join(sentence)) # turns tokens into a string\n",
    "        \n",
    "        return(sentences)\n",
    "    \n",
    "    def find_common(self, elems, top=None) -> list:\n",
    "        \"\"\"\n",
    "            receives list of elements\n",
    "            returns a list of counters\n",
    "            [(elem, 7)]\n",
    "        \"\"\"\n",
    "        return(Counter(elems).most_common(top))\n",
    "    \n",
    "    def getStems(self, tokens) -> list:\n",
    "        \"\"\"\n",
    "            get [(word: stem)] using SnowballStemmer by nltk\n",
    "        \"\"\"\n",
    "        stemmer = SnowballStemmer('russian')\n",
    "        return([(word, stemmer.stem(word)) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "Отчистите текст от мусора (тэгов, хешей и тп) с помощью регулярных выражений. Постарайтесь убрать весь мусор, но если что-то удалить не получается, то не мучайтесь. Главное, чтобы мусор не проявлялся в результатах следующих заданий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Борис Леонидович Пастернак. Доктор Живаго. «Доктор Живаго» - итоговое произведение Бориса Пастернака, книга всей его жизни. Этот роман принес его автору мировую известность и Нобелевскую премию, присуждение которой обернулось для поэта оголтелой политической травлей, обвинениями в «измене Родине» и в результате стоило ему жизни.. «Доктор Живаго» - роман, сама ткань которого убедительнее свидетельствует о чуде, чем все размышления доктора и обобщения автора. Человек, который так пишет, бесконечн\n",
      "\n",
      "\n",
      "тынь. Сейчас должно написанное сбыться, Пускай же сбудется оно. Аминь. Ты видишь, ход веков подобен притче И может загореться на ходу. Во имя страшного ее величья Я в добровольных муках в гроб сойду. Я в гроб сойду и в третий день восстану, И, как сплавляют по реке плоты, Ко мне на суд, как баржи каравана, Столетья поплывут из темноты».. Примечания. В восторге. (Здесь и далее с французского.). Большой круг! Китайская цепочка!. Вальс, пожалуйста!. На три счета, на два счета.. Шиворот-навыворот..\n"
     ]
    }
   ],
   "source": [
    "preproc = Preprocessor('zhivago.txt')\n",
    "preproc.getFileContent()\n",
    "preproc.removeJunk()\n",
    "print(preproc.clean[0:500])\n",
    "print('\\n')\n",
    "print(preproc.clean[-500:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "Приведите очищенный текст к нижнему регистру, удалите все знаки пунктуации, разделите на предложения библиотекой rusenttokenize, токенизируйте библиотекой razdel_tokenize. \n",
    "Ответьте на следующие вопросы:\n",
    "\n",
    "1) есть ли в тексте повторяющиеся корректные предложения? если да то какие? (если находится мусор то вернитесь к заданию 1 и постарайтесь избавиться от него)\n",
    "\n",
    "2) какой самый частотный токен в тексте длиннее 6 символов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    }
   ],
   "source": [
    "tokens = preproc.tokenize(preproc.clean)\n",
    "sentences = preproc.getSentences(preproc.clean)\n",
    "tokens_by_sentence = [preproc.tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Борис Леонидович Пастернак.', 'Доктор Живаго.', '«Доктор Живаго» - итоговое произведение Бориса Пастернака, книга всей его жизни.', 'Этот роман принес его автору мировую известность и Нобелевскую премию, присуждение которой обернулось для поэта оголтелой политической травлей, обвинениями в «измене Родине» и в результате стоило ему жизни..']\n",
      "[['борис', 'леонидович', 'пастернак'], ['доктор', 'живаго']]\n",
      "['борис', 'леонидович', 'пастернак', 'доктор', 'живаго']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:4])\n",
    "print(tokens_by_sentence[0:2])\n",
    "print(tokens[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common token longer than 6\n",
    "common_token = preproc.find_common(\n",
    "    preproc.filter_tokens_by_length(tokens, '>', 6),\n",
    "    1)\n",
    "# common sentences\n",
    "common_sentence = preproc.find_common(\n",
    "    preproc.group_sentences(\n",
    "        preproc.filter_tokens_by_length(tokens_by_sentence, '>', 6)\n",
    "    ), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('единственно живое и яркое в вас это то что вы жили в одно время со мной и меня знали', 2)]\n",
      "[('андреевич', 289)]\n"
     ]
    }
   ],
   "source": [
    "print(common_sentence)\n",
    "print(common_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Сделайте стемминг и найдите по несколько частотных примеров на каждый из видов ошибок:\n",
    "1) два разных слова ошибочно свелись к одинаковой основе\n",
    "('прем', ['премию', 'премило'], ('лебед', ['лебедью', 'лебедь', 'лебеды'])\n",
    "\n",
    "2) слово не изменилось после стемминга (слово должно быть русским и длиннее 4 символов) \n",
    "('дружок', 'дружок'), ('грешат', 'грешат')\n",
    "\n",
    "***не делайте это задание полностью вручную, напишите правила, которые отловят потенциальные ошибки и выберите из них***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = preproc.getStems(preproc.tokenize(preproc.clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('прем', ['премию', 'премило'])\n"
     ]
    }
   ],
   "source": [
    "false_relatives = {} # {stem: [tokens]}\n",
    "\n",
    "for stem in stems:\n",
    "    if stem[1] in false_relatives.keys():\n",
    "        false_relatives[stem[1]].append(stem[0])\n",
    "        continue\n",
    "    false_relatives[stem[1]] = stem[0].split()\n",
    "\n",
    "filtered = []\n",
    "\n",
    "for stem, forms in false_relatives.items():\n",
    "    if len(forms) > 1:\n",
    "        if len(set(forms)) == len(forms):\n",
    "            filtered.append((stem, forms))\n",
    "\n",
    "print(filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('дружок', 'дружок')\n"
     ]
    }
   ],
   "source": [
    "no_change = [stem for stem in stems if stem[0] == stem[1] and len(stem[0]) > 4]\n",
    "print(no_change[525])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Проанализируйте список стоп-слов из нлтк (для русского). Какие ещё слова вы бы туда добавили? (5 слов будет достаточно, не забудьте аргументировать ваш выбор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('russian')\n",
    "\n",
    "\"\"\"\n",
    "# опираясь на текст пастернака можно добавить \n",
    "# некоторые из 200 наиболее употребимых токенов из произведения\n",
    "# будет зависеть от анализируемого текста и задач\n",
    "\"\"\"\n",
    "to_add = [\"пока\", # 93\n",
    "          \"который\", # 104\n",
    "          \"что-то\", # 123\n",
    "          \"нам\", # 66\n",
    "          \"несколько\", # 115\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "\n",
    "Предобработайте текст двумя способами:\n",
    "1) лемматизируйте токены с помощью pymorphy2\n",
    "2) лемматизируйте текст с помощью mystem3 \n",
    "\n",
    "Ответьте на вопрос:\n",
    "Что в данном случае лучше для лемматизации mystem или pymorphy?\n",
    "\n",
    "Важно, чтобы ответы на вопросы были аргументированы (как минимум три аргумента).  Анализируйте результаты с языковой точки зрения. Скорость работы простота интерфейса не являются преимуществами или недостатками в рамках этого задания. Идеальный аргумент - библиотека x неправильно обрабатывает вот такое слово/класс слов.\n",
    "\n",
    "### Ответ:\n",
    "\n",
    "MyStem лучше справляется с существительными\n",
    "1) слово \"суть\" остается существительным, тогда как PyMorphy переводит его в инфинитив \"быть\"\n",
    "2) слово \"денег\" переходит в \"деньги\", тогда как PyMorphy переводит его в \"деньга\"\n",
    "3) слово \"неволю\" переходит в \"неволя\", тогда как PyMorphy переводит его в инфинитив \"неволить\"\n",
    "4) слово \"иней\" переходит в \"иней\", тогда как PyMorphy переводит его в \"иня\"\n",
    "\n",
    "MyStem приводит глаголы к несовершенному виду, тогда как PyMorphy к совершенному (скорее замечание, а не проблема)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morpher():\n",
    "    \n",
    "    def __init__(self, morph_type):\n",
    "        if morph_type == 'pymorphy2':\n",
    "            self.morpho = MorphAnalyzer() \n",
    "            self.cash = {}\n",
    "            self.analyze = self.usePymorphy\n",
    "\n",
    "        elif morph_type == 'mystem':\n",
    "            self.mystem = Mystem()\n",
    "            self.analyze = self.useMystem\n",
    "        else:\n",
    "            return(ValueError)\n",
    "        \n",
    "        self.mode = morph_type\n",
    "            \n",
    "    def useMystem(self, words) -> list:\n",
    "        \"\"\" \n",
    "            receives list of raw words\n",
    "        \"\"\"\n",
    "        text = ' '.join(words)\n",
    "        tokens = self.mystem.analyze(text)\n",
    "        res = {}\n",
    "        for t in tokens:\n",
    "            if 'analysis' in t.keys():\n",
    "                if t['analysis'] != []:\n",
    "                    res[t['text']] = t['analysis'][0]['lex']\n",
    "                else:\n",
    "                    res[t['text']] = t['text']\n",
    "        return(res)    \n",
    "\n",
    "    def usePymorphy(self, words) -> list:\n",
    "        \"\"\" \n",
    "            receives list of raw words\n",
    "        \"\"\"\n",
    "        res={}\n",
    "        for w in words:\n",
    "            if w in self.cash:\n",
    "                res[w] = self.cash[w]\n",
    "            else:\n",
    "                r=self.morpho.parse(w)[0].normal_form\n",
    "                res[w] = r\n",
    "                self.cash[w]=r\n",
    "        return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorpher = Morpher('pymorphy2')\n",
    "mymorpher = Morpher('mystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_by_pymorphy = pymorpher.analyze(tokens[0:10000])\n",
    "tokens_by_mystem = mymorpher.analyze(tokens[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'борис': 'борис', 'леонидович': 'леонидович', 'пастернак': 'пастернак', 'доктор': 'доктор', 'живаго': 'живаго', 'итоговое': 'итоговый', 'произведение': 'произведение', 'бориса': 'борис'}\n",
      "{'борис': 'борис', 'леонидович': 'леонидович', 'пастернак': 'пастернак', 'доктор': 'доктор', 'живаго': 'живаго', 'итоговое': 'итоговый', 'произведение': 'произведение', 'бориса': 'борис'}\n"
     ]
    }
   ],
   "source": [
    "print(tokens_by_pymorphy)\n",
    "print(tokens_by_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = {}\n",
    "for k, v in tokens_by_pymorphy.items():\n",
    "    try:\n",
    "        if tokens_by_mystem[k] != v:\n",
    "            difference[k + \"_\" + 'PM'] = v\n",
    "            difference[k + \"_\" + 'MS'] = tokens_by_mystem[k]\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
